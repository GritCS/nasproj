heming-g3 :: ~/Single-Path-One-Shot-NAS ‹master*› » CUDA_VISIBLE_DEVICES=2 python3 supernet.py --exp_name=spos_cifar10        1 ↵
Namespace(auto_aug=False, batch_size=96, classes=10, cutout=False, cutout_length=16, data_dir='/home/work/dataset/', dataset='cifar10', epochs=600, exp_name='spos_cifar10', layers=20, learning_rate=0.025, momentum=0.9, num_choices=4, random_search=1000, resize=False, val_interval=5, weight_decay=0.0003)
Train on GPU!
Files already downloaded and verified
Files already downloaded and verified
Random Path of the Supernet: Params: 2.35M, Flops:191.07M
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 16, 32, 32]             432
       BatchNorm2d-2           [-1, 16, 32, 32]               0
             ReLU6-3           [-1, 16, 32, 32]               0
            Conv2d-4           [-1, 32, 32, 32]             256
       BatchNorm2d-5           [-1, 32, 32, 32]               0
              ReLU-6           [-1, 32, 32, 32]               0
            Conv2d-7           [-1, 32, 32, 32]           1,568
       BatchNorm2d-8           [-1, 32, 32, 32]               0
            Conv2d-9           [-1, 56, 32, 32]           1,792
      BatchNorm2d-10           [-1, 56, 32, 32]               0
             ReLU-11           [-1, 56, 32, 32]               0
     Choice_Block-12           [-1, 64, 32, 32]               0
           Conv2d-13           [-1, 32, 32, 32]             288
      BatchNorm2d-14           [-1, 32, 32, 32]               0
           Conv2d-15           [-1, 32, 32, 32]           1,024
      BatchNorm2d-16           [-1, 32, 32, 32]               0
             ReLU-17           [-1, 32, 32, 32]               0
           Conv2d-18           [-1, 32, 32, 32]             288
      BatchNorm2d-19           [-1, 32, 32, 32]               0
           Conv2d-20           [-1, 32, 32, 32]           1,024
      BatchNorm2d-21           [-1, 32, 32, 32]               0
             ReLU-22           [-1, 32, 32, 32]               0
           Conv2d-23           [-1, 32, 32, 32]             288
      BatchNorm2d-24           [-1, 32, 32, 32]               0
           Conv2d-25           [-1, 32, 32, 32]           1,024
      BatchNorm2d-26           [-1, 32, 32, 32]               0
             ReLU-27           [-1, 32, 32, 32]               0
   Choice_Block_x-28           [-1, 64, 32, 32]               0
           Conv2d-29           [-1, 32, 32, 32]             288
      BatchNorm2d-30           [-1, 32, 32, 32]               0
           Conv2d-31           [-1, 32, 32, 32]           1,024
      BatchNorm2d-32           [-1, 32, 32, 32]               0
             ReLU-33           [-1, 32, 32, 32]               0
           Conv2d-34           [-1, 32, 32, 32]             288
      BatchNorm2d-35           [-1, 32, 32, 32]               0
           Conv2d-36           [-1, 32, 32, 32]           1,024
      BatchNorm2d-37           [-1, 32, 32, 32]               0
             ReLU-38           [-1, 32, 32, 32]               0
           Conv2d-39           [-1, 32, 32, 32]             288
      BatchNorm2d-40           [-1, 32, 32, 32]               0
           Conv2d-41           [-1, 32, 32, 32]           1,024
      BatchNorm2d-42           [-1, 32, 32, 32]               0
             ReLU-43           [-1, 32, 32, 32]               0
   Choice_Block_x-44           [-1, 64, 32, 32]               0
           Conv2d-45           [-1, 32, 32, 32]           1,024
      BatchNorm2d-46           [-1, 32, 32, 32]               0
             ReLU-47           [-1, 32, 32, 32]               0
           Conv2d-48           [-1, 32, 32, 32]           1,568
      BatchNorm2d-49           [-1, 32, 32, 32]               0
           Conv2d-50           [-1, 32, 32, 32]           1,024
      BatchNorm2d-51           [-1, 32, 32, 32]               0
             ReLU-52           [-1, 32, 32, 32]               0
     Choice_Block-53           [-1, 64, 32, 32]               0
           Conv2d-54           [-1, 80, 32, 32]           5,120
      BatchNorm2d-55           [-1, 80, 32, 32]               0
             ReLU-56           [-1, 80, 32, 32]               0
           Conv2d-57           [-1, 80, 16, 16]           3,920
      BatchNorm2d-58           [-1, 80, 16, 16]               0
           Conv2d-59           [-1, 96, 16, 16]           7,680
      BatchNorm2d-60           [-1, 96, 16, 16]               0
             ReLU-61           [-1, 96, 16, 16]               0
           Conv2d-62           [-1, 64, 16, 16]           3,136
      BatchNorm2d-63           [-1, 64, 16, 16]               0
           Conv2d-64           [-1, 64, 16, 16]           4,096
      BatchNorm2d-65           [-1, 64, 16, 16]               0
             ReLU-66           [-1, 64, 16, 16]               0
     Choice_Block-67          [-1, 160, 16, 16]               0
           Conv2d-68           [-1, 80, 16, 16]             720
      BatchNorm2d-69           [-1, 80, 16, 16]               0
           Conv2d-70           [-1, 80, 16, 16]           6,400
      BatchNorm2d-71           [-1, 80, 16, 16]               0
             ReLU-72           [-1, 80, 16, 16]               0
           Conv2d-73           [-1, 80, 16, 16]             720
      BatchNorm2d-74           [-1, 80, 16, 16]               0
           Conv2d-75           [-1, 80, 16, 16]           6,400
      BatchNorm2d-76           [-1, 80, 16, 16]               0
             ReLU-77           [-1, 80, 16, 16]               0
           Conv2d-78           [-1, 80, 16, 16]             720
      BatchNorm2d-79           [-1, 80, 16, 16]               0
           Conv2d-80           [-1, 80, 16, 16]           6,400
      BatchNorm2d-81           [-1, 80, 16, 16]               0
             ReLU-82           [-1, 80, 16, 16]               0
   Choice_Block_x-83          [-1, 160, 16, 16]               0
           Conv2d-84           [-1, 80, 16, 16]           6,400
      BatchNorm2d-85           [-1, 80, 16, 16]               0
             ReLU-86           [-1, 80, 16, 16]               0
           Conv2d-87           [-1, 80, 16, 16]             720
      BatchNorm2d-88           [-1, 80, 16, 16]               0
           Conv2d-89           [-1, 80, 16, 16]           6,400
      BatchNorm2d-90           [-1, 80, 16, 16]               0
             ReLU-91           [-1, 80, 16, 16]               0
     Choice_Block-92          [-1, 160, 16, 16]               0
           Conv2d-93           [-1, 80, 16, 16]           6,400
      BatchNorm2d-94           [-1, 80, 16, 16]               0
             ReLU-95           [-1, 80, 16, 16]               0
           Conv2d-96           [-1, 80, 16, 16]           2,000
      BatchNorm2d-97           [-1, 80, 16, 16]               0
           Conv2d-98           [-1, 80, 16, 16]           6,400
      BatchNorm2d-99           [-1, 80, 16, 16]               0
            ReLU-100           [-1, 80, 16, 16]               0
    Choice_Block-101          [-1, 160, 16, 16]               0
          Conv2d-102          [-1, 160, 16, 16]          25,600
     BatchNorm2d-103          [-1, 160, 16, 16]               0
            ReLU-104          [-1, 160, 16, 16]               0
          Conv2d-105            [-1, 160, 8, 8]           7,840
     BatchNorm2d-106            [-1, 160, 8, 8]               0
          Conv2d-107            [-1, 160, 8, 8]          25,600
     BatchNorm2d-108            [-1, 160, 8, 8]               0
            ReLU-109            [-1, 160, 8, 8]               0
          Conv2d-110            [-1, 160, 8, 8]           7,840
     BatchNorm2d-111            [-1, 160, 8, 8]               0
          Conv2d-112            [-1, 160, 8, 8]          25,600
     BatchNorm2d-113            [-1, 160, 8, 8]               0
            ReLU-114            [-1, 160, 8, 8]               0
    Choice_Block-115            [-1, 320, 8, 8]               0
          Conv2d-116            [-1, 160, 8, 8]          25,600
     BatchNorm2d-117            [-1, 160, 8, 8]               0
            ReLU-118            [-1, 160, 8, 8]               0
          Conv2d-119            [-1, 160, 8, 8]           4,000
     BatchNorm2d-120            [-1, 160, 8, 8]               0
          Conv2d-121            [-1, 160, 8, 8]          25,600
     BatchNorm2d-122            [-1, 160, 8, 8]               0
            ReLU-123            [-1, 160, 8, 8]               0
    Choice_Block-124            [-1, 320, 8, 8]               0
          Conv2d-125            [-1, 160, 8, 8]          25,600
     BatchNorm2d-126            [-1, 160, 8, 8]               0
            ReLU-127            [-1, 160, 8, 8]               0
          Conv2d-128            [-1, 160, 8, 8]           4,000
     BatchNorm2d-129            [-1, 160, 8, 8]               0
          Conv2d-130            [-1, 160, 8, 8]          25,600
     BatchNorm2d-131            [-1, 160, 8, 8]               0
            ReLU-132            [-1, 160, 8, 8]               0
    Choice_Block-133            [-1, 320, 8, 8]               0
          Conv2d-134            [-1, 160, 8, 8]          25,600
     BatchNorm2d-135            [-1, 160, 8, 8]               0
            ReLU-136            [-1, 160, 8, 8]               0
          Conv2d-137            [-1, 160, 8, 8]           7,840
     BatchNorm2d-138            [-1, 160, 8, 8]               0
          Conv2d-139            [-1, 160, 8, 8]          25,600
     BatchNorm2d-140            [-1, 160, 8, 8]               0
            ReLU-141            [-1, 160, 8, 8]               0
    Choice_Block-142            [-1, 320, 8, 8]               0
          Conv2d-143            [-1, 160, 8, 8]           1,440
     BatchNorm2d-144            [-1, 160, 8, 8]               0
          Conv2d-145            [-1, 160, 8, 8]          25,600
     BatchNorm2d-146            [-1, 160, 8, 8]               0
            ReLU-147            [-1, 160, 8, 8]               0
          Conv2d-148            [-1, 160, 8, 8]           1,440
     BatchNorm2d-149            [-1, 160, 8, 8]               0
          Conv2d-150            [-1, 160, 8, 8]          25,600
     BatchNorm2d-151            [-1, 160, 8, 8]               0
            ReLU-152            [-1, 160, 8, 8]               0
          Conv2d-153            [-1, 160, 8, 8]           1,440
     BatchNorm2d-154            [-1, 160, 8, 8]               0
          Conv2d-155            [-1, 160, 8, 8]          25,600
     BatchNorm2d-156            [-1, 160, 8, 8]               0
            ReLU-157            [-1, 160, 8, 8]               0
  Choice_Block_x-158            [-1, 320, 8, 8]               0
          Conv2d-159            [-1, 160, 8, 8]          25,600
     BatchNorm2d-160            [-1, 160, 8, 8]               0
            ReLU-161            [-1, 160, 8, 8]               0
          Conv2d-162            [-1, 160, 8, 8]           4,000
     BatchNorm2d-163            [-1, 160, 8, 8]               0
          Conv2d-164            [-1, 160, 8, 8]          25,600
     BatchNorm2d-165            [-1, 160, 8, 8]               0
            ReLU-166            [-1, 160, 8, 8]               0
    Choice_Block-167            [-1, 320, 8, 8]               0
          Conv2d-168            [-1, 160, 8, 8]          25,600
     BatchNorm2d-169            [-1, 160, 8, 8]               0
            ReLU-170            [-1, 160, 8, 8]               0
          Conv2d-171            [-1, 160, 8, 8]           4,000
     BatchNorm2d-172            [-1, 160, 8, 8]               0
          Conv2d-173            [-1, 160, 8, 8]          25,600
     BatchNorm2d-174            [-1, 160, 8, 8]               0
            ReLU-175            [-1, 160, 8, 8]               0
    Choice_Block-176            [-1, 320, 8, 8]               0
          Conv2d-177            [-1, 160, 8, 8]           1,440
     BatchNorm2d-178            [-1, 160, 8, 8]               0
          Conv2d-179            [-1, 160, 8, 8]          25,600
     BatchNorm2d-180            [-1, 160, 8, 8]               0
            ReLU-181            [-1, 160, 8, 8]               0
          Conv2d-182            [-1, 160, 8, 8]           1,440
     BatchNorm2d-183            [-1, 160, 8, 8]               0
          Conv2d-184            [-1, 160, 8, 8]          25,600
     BatchNorm2d-185            [-1, 160, 8, 8]               0
            ReLU-186            [-1, 160, 8, 8]               0
          Conv2d-187            [-1, 160, 8, 8]           1,440
     BatchNorm2d-188            [-1, 160, 8, 8]               0
          Conv2d-189            [-1, 160, 8, 8]          25,600
     BatchNorm2d-190            [-1, 160, 8, 8]               0
            ReLU-191            [-1, 160, 8, 8]               0
  Choice_Block_x-192            [-1, 320, 8, 8]               0
          Conv2d-193            [-1, 160, 8, 8]           1,440
     BatchNorm2d-194            [-1, 160, 8, 8]               0
          Conv2d-195            [-1, 320, 8, 8]          51,200
     BatchNorm2d-196            [-1, 320, 8, 8]               0
            ReLU-197            [-1, 320, 8, 8]               0
          Conv2d-198            [-1, 320, 8, 8]           2,880
     BatchNorm2d-199            [-1, 320, 8, 8]               0
          Conv2d-200            [-1, 320, 8, 8]         102,400
     BatchNorm2d-201            [-1, 320, 8, 8]               0
            ReLU-202            [-1, 320, 8, 8]               0
          Conv2d-203            [-1, 320, 8, 8]           2,880
     BatchNorm2d-204            [-1, 320, 8, 8]               0
          Conv2d-205            [-1, 480, 8, 8]         153,600
     BatchNorm2d-206            [-1, 480, 8, 8]               0
            ReLU-207            [-1, 480, 8, 8]               0
  Choice_Block_x-208            [-1, 640, 8, 8]               0
          Conv2d-209            [-1, 320, 8, 8]           2,880
     BatchNorm2d-210            [-1, 320, 8, 8]               0
          Conv2d-211            [-1, 320, 8, 8]         102,400
     BatchNorm2d-212            [-1, 320, 8, 8]               0
            ReLU-213            [-1, 320, 8, 8]               0
          Conv2d-214            [-1, 320, 8, 8]           2,880
     BatchNorm2d-215            [-1, 320, 8, 8]               0
          Conv2d-216            [-1, 320, 8, 8]         102,400
     BatchNorm2d-217            [-1, 320, 8, 8]               0
            ReLU-218            [-1, 320, 8, 8]               0
          Conv2d-219            [-1, 320, 8, 8]           2,880
     BatchNorm2d-220            [-1, 320, 8, 8]               0
          Conv2d-221            [-1, 320, 8, 8]         102,400
     BatchNorm2d-222            [-1, 320, 8, 8]               0
            ReLU-223            [-1, 320, 8, 8]               0
  Choice_Block_x-224            [-1, 640, 8, 8]               0
          Conv2d-225            [-1, 320, 8, 8]         102,400
     BatchNorm2d-226            [-1, 320, 8, 8]               0
            ReLU-227            [-1, 320, 8, 8]               0
          Conv2d-228            [-1, 320, 8, 8]           8,000
     BatchNorm2d-229            [-1, 320, 8, 8]               0
          Conv2d-230            [-1, 320, 8, 8]         102,400
     BatchNorm2d-231            [-1, 320, 8, 8]               0
            ReLU-232            [-1, 320, 8, 8]               0
    Choice_Block-233            [-1, 640, 8, 8]               0
          Conv2d-234            [-1, 320, 8, 8]         102,400
     BatchNorm2d-235            [-1, 320, 8, 8]               0
            ReLU-236            [-1, 320, 8, 8]               0
          Conv2d-237            [-1, 320, 8, 8]           8,000
     BatchNorm2d-238            [-1, 320, 8, 8]               0
          Conv2d-239            [-1, 320, 8, 8]         102,400
     BatchNorm2d-240            [-1, 320, 8, 8]               0
            ReLU-241            [-1, 320, 8, 8]               0
    Choice_Block-242            [-1, 640, 8, 8]               0
          Conv2d-243           [-1, 1024, 8, 8]         655,360
     BatchNorm2d-244           [-1, 1024, 8, 8]               0
           ReLU6-245           [-1, 1024, 8, 8]               0
AdaptiveAvgPool2d-246           [-1, 1024, 1, 1]               0
          Linear-247                   [-1, 10]          10,240
================================================================
Total params: 2,345,168
Trainable params: 2,345,168
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 42.37
Params size (MB): 8.95
Estimated Total Size (MB): 51.33
----------------------------------------------------------------
  0%|                                                                                                     | 0/521 [00:00<?, ?it/s]/home/shixiong/.local/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
[Epoch:0001/0600 lr:0.025000]: 100%|██| 521/521 [00:34<00:00, 14.95it/s, log={'train_loss': '2.211836', 'train_acc': '15.157999'}]
[Epoch:0002/0600 lr:0.024958]: 100%|██| 521/521 [00:36<00:00, 14.08it/s, log={'train_loss': '1.975998', 'train_acc': '24.095999'}]
[Epoch:0003/0600 lr:0.024917]: 100%|██| 521/521 [00:37<00:00, 13.96it/s, log={'train_loss': '1.844755', 'train_acc': '28.883999'}]
[Epoch:0004/0600 lr:0.024875]: 100%|██| 521/521 [00:36<00:00, 14.14it/s, log={'train_loss': '1.783350', 'train_acc': '31.857999'}]
[Epoch:0005/0600 lr:0.024833]: 100%|██| 521/521 [00:36<00:00, 14.41it/s, log={'train_loss': '1.727439', 'train_acc': '34.313999'}]
[Val_Accuracy epoch:5] val_loss:1.703520, val_acc:34.939999
[Epoch:0006/0600 lr:0.024792]: 100%|██| 521/521 [00:36<00:00, 14.34it/s, log={'train_loss': '1.674344', 'train_acc': '36.873999'}]
[Epoch:0007/0600 lr:0.024750]: 100%|██| 521/521 [00:37<00:00, 13.98it/s, log={'train_loss': '1.633087', 'train_acc': '38.667999'}]
[Epoch:0008/0600 lr:0.024708]: 100%|██| 521/521 [00:37<00:00, 13.93it/s, log={'train_loss': '1.598780', 'train_acc': '40.241999'}]
[Epoch:0009/0600 lr:0.024667]: 100%|██| 521/521 [00:36<00:00, 14.13it/s, log={'train_loss': '1.564958', 'train_acc': '41.885999'}]
[Epoch:0010/0600 lr:0.024625]: 100%|██| 521/521 [00:35<00:00, 14.74it/s, log={'train_loss': '1.532624', 'train_acc': '43.243999'}]
[Val_Accuracy epoch:10] val_loss:1.478384, val_acc:45.259998
[Epoch:0011/0600 lr:0.024583]: 100%|██| 521/521 [00:37<00:00, 13.90it/s, log={'train_loss': '1.504401', 'train_acc': '44.411999'}]
[Epoch:0012/0600 lr:0.024542]: 100%|██| 521/521 [00:37<00:00, 13.77it/s, log={'train_loss': '1.480646', 'train_acc': '45.615999'}]
[Epoch:0013/0600 lr:0.024500]: 100%|██| 521/521 [00:37<00:00, 13.97it/s, log={'train_loss': '1.450325', 'train_acc': '47.119999'}]
[Epoch:0014/0600 lr:0.024458]: 100%|██| 521/521 [00:36<00:00, 14.33it/s, log={'train_loss': '1.422941', 'train_acc': '47.985998'}]
[Epoch:0015/0600 lr:0.024417]: 100%|██| 521/521 [00:37<00:00, 13.86it/s, log={'train_loss': '1.398401', 'train_acc': '48.823998'}]
[Val_Accuracy epoch:15] val_loss:1.439384, val_acc:47.909999
[Epoch:0016/0600 lr:0.024375]: 100%|██| 521/521 [00:36<00:00, 14.33it/s, log={'train_loss': '1.376279', 'train_acc': '49.683998'}]
[Epoch:0017/0600 lr:0.024333]: 100%|██| 521/521 [00:37<00:00, 13.83it/s, log={'train_loss': '1.349691', 'train_acc': '50.877998'}]
[Epoch:0018/0600 lr:0.024292]: 100%|██| 521/521 [00:35<00:00, 14.58it/s, log={'train_loss': '1.328420', 'train_acc': '51.657998'}]
[Epoch:0019/0600 lr:0.024250]: 100%|██| 521/521 [00:37<00:00, 13.88it/s, log={'train_loss': '1.300636', 'train_acc': '52.545998'}]
[Epoch:0020/0600 lr:0.024208]: 100%|██| 521/521 [00:37<00:00, 13.97it/s, log={'train_loss': '1.277492', 'train_acc': '53.915998'}]
[Val_Accuracy epoch:20] val_loss:1.284729, val_acc:52.369998
[Epoch:0021/0600 lr:0.024167]: 100%|██| 521/521 [00:37<00:00, 13.87it/s, log={'train_loss': '1.255411', 'train_acc': '54.505998'}]
[Epoch:0022/0600 lr:0.024125]: 100%|██| 521/521 [00:36<00:00, 14.35it/s, log={'train_loss': '1.232914', 'train_acc': '55.315998'}]
[Epoch:0023/0600 lr:0.024083]: 100%|██| 521/521 [00:38<00:00, 13.68it/s, log={'train_loss': '1.212500', 'train_acc': '56.297998'}]
[Epoch:0024/0600 lr:0.024042]: 100%|██| 521/521 [00:36<00:00, 14.45it/s, log={'train_loss': '1.192204', 'train_acc': '57.101998'}]
[Epoch:0025/0600 lr:0.024000]: 100%|██| 521/521 [00:36<00:00, 14.19it/s, log={'train_loss': '1.174412', 'train_acc': '57.599998'}]
[Val_Accuracy epoch:25] val_loss:1.259369, val_acc:53.709998
[Epoch:0026/0600 lr:0.023958]: 100%|██| 521/521 [00:37<00:00, 13.81it/s, log={'train_loss': '1.150869', 'train_acc': '58.639997'}]
[Epoch:0027/0600 lr:0.023917]: 100%|██| 521/521 [00:36<00:00, 14.13it/s, log={'train_loss': '1.133577', 'train_acc': '59.285998'}]
[Epoch:0028/0600 lr:0.023875]: 100%|██| 521/521 [00:37<00:00, 13.98it/s, log={'train_loss': '1.115777', 'train_acc': '60.003997'}]
[Epoch:0029/0600 lr:0.023833]: 100%|██| 521/521 [00:36<00:00, 14.46it/s, log={'train_loss': '1.097154', 'train_acc': '60.643997'}]
[Epoch:0030/0600 lr:0.023792]: 100%|██| 521/521 [00:36<00:00, 14.30it/s, log={'train_loss': '1.073141', 'train_acc': '61.601997'}]
[Val_Accuracy epoch:30] val_loss:1.153187, val_acc:59.579997
[Epoch:0031/0600 lr:0.023750]: 100%|██| 521/521 [00:35<00:00, 14.50it/s, log={'train_loss': '1.062482', 'train_acc': '61.887997'}]
[Epoch:0032/0600 lr:0.023708]: 100%|██| 521/521 [00:36<00:00, 14.37it/s, log={'train_loss': '1.042354', 'train_acc': '62.723997'}]
[Epoch:0033/0600 lr:0.023667]: 100%|██| 521/521 [00:37<00:00, 13.84it/s, log={'train_loss': '1.026938', 'train_acc': '63.137997'}]
[Epoch:0034/0600 lr:0.023625]: 100%|██| 521/521 [00:37<00:00, 13.95it/s, log={'train_loss': '1.010105', 'train_acc': '64.075997'}]
[Epoch:0035/0600 lr:0.023583]: 100%|██| 521/521 [00:37<00:00, 13.93it/s, log={'train_loss': '0.994528', 'train_acc': '64.511997'}]
[Val_Accuracy epoch:35] val_loss:1.007838, val_acc:64.879997
[Epoch:0036/0600 lr:0.023542]: 100%|██| 521/521 [00:37<00:00, 13.86it/s, log={'train_loss': '0.979752', 'train_acc': '65.233997'}]
[Epoch:0037/0600 lr:0.023500]: 100%|██| 521/521 [00:37<00:00, 13.99it/s, log={'train_loss': '0.958274', 'train_acc': '66.013997'}]
[Epoch:0038/0600 lr:0.023458]: 100%|██| 521/521 [00:37<00:00, 13.91it/s, log={'train_loss': '0.950823', 'train_acc': '66.359997'}]
[Epoch:0039/0600 lr:0.023417]: 100%|██| 521/521 [00:37<00:00, 13.85it/s, log={'train_loss': '0.943204', 'train_acc': '66.317997'}]
[Epoch:0040/0600 lr:0.023375]: 100%|██| 521/521 [00:38<00:00, 13.66it/s, log={'train_loss': '0.921372', 'train_acc': '67.329997'}]
[Val_Accuracy epoch:40] val_loss:0.991121, val_acc:65.309998
[Epoch:0041/0600 lr:0.023333]: 100%|██| 521/521 [00:37<00:00, 13.97it/s, log={'train_loss': '0.913050', 'train_acc': '67.621997'}]
[Epoch:0042/0600 lr:0.023292]: 100%|██| 521/521 [00:38<00:00, 13.69it/s, log={'train_loss': '0.893222', 'train_acc': '68.137997'}]
[Epoch:0043/0600 lr:0.023250]: 100%|██| 521/521 [00:37<00:00, 13.88it/s, log={'train_loss': '0.884839', 'train_acc': '68.565997'}]
[Epoch:0044/0600 lr:0.023208]: 100%|██| 521/521 [00:38<00:00, 13.64it/s, log={'train_loss': '0.875784', 'train_acc': '68.911997'}]
[Epoch:0045/0600 lr:0.023167]: 100%|██| 521/521 [00:38<00:00, 13.58it/s, log={'train_loss': '0.866702', 'train_acc': '69.047997'}]
[Val_Accuracy epoch:45] val_loss:0.871115, val_acc:68.509997
[Epoch:0046/0600 lr:0.023125]: 100%|██| 521/521 [00:36<00:00, 14.08it/s, log={'train_loss': '0.850560', 'train_acc': '69.925997'}]
[Epoch:0047/0600 lr:0.023083]: 100%|██| 521/521 [00:35<00:00, 14.58it/s, log={'train_loss': '0.844149', 'train_acc': '70.015997'}]
[Epoch:0048/0600 lr:0.023042]: 100%|██| 521/521 [00:36<00:00, 14.14it/s, log={'train_loss': '0.832338', 'train_acc': '70.303997'}]
[Epoch:0049/0600 lr:0.023000]: 100%|██| 521/521 [00:36<00:00, 14.29it/s, log={'train_loss': '0.823798', 'train_acc': '70.879997'}]
[Epoch:0050/0600 lr:0.022958]: 100%|██| 521/521 [00:38<00:00, 13.64it/s, log={'train_loss': '0.814648', 'train_acc': '71.341997'}]
[Val_Accuracy epoch:50] val_loss:0.849288, val_acc:70.629998
[Epoch:0051/0600 lr:0.022917]: 100%|██| 521/521 [00:38<00:00, 13.56it/s, log={'train_loss': '0.807349', 'train_acc': '71.393997'}]
[Epoch:0052/0600 lr:0.022875]: 100%|██| 521/521 [00:37<00:00, 13.84it/s, log={'train_loss': '0.792917', 'train_acc': '71.791997'}]
[Epoch:0053/0600 lr:0.022833]: 100%|██| 521/521 [00:38<00:00, 13.63it/s, log={'train_loss': '0.786858', 'train_acc': '72.161997'}]
[Epoch:0054/0600 lr:0.022792]: 100%|██| 521/521 [00:37<00:00, 13.91it/s, log={'train_loss': '0.776761', 'train_acc': '72.455997'}]
[Epoch:0055/0600 lr:0.022750]: 100%|██| 521/521 [00:36<00:00, 14.12it/s, log={'train_loss': '0.768269', 'train_acc': '72.653998'}]
[Val_Accuracy epoch:55] val_loss:0.771406, val_acc:72.869998
[Epoch:0056/0600 lr:0.022708]: 100%|██| 521/521 [00:37<00:00, 13.82it/s, log={'train_loss': '0.759734', 'train_acc': '73.117997'}]
[Epoch:0057/0600 lr:0.022667]: 100%|██| 521/521 [00:36<00:00, 14.31it/s, log={'train_loss': '0.748476', 'train_acc': '73.575997'}]
[Epoch:0058/0600 lr:0.022625]: 100%|██| 521/521 [00:36<00:00, 14.29it/s, log={'train_loss': '0.742937', 'train_acc': '73.689998'}]
[Epoch:0059/0600 lr:0.022583]: 100%|██| 521/521 [00:37<00:00, 13.89it/s, log={'train_loss': '0.733610', 'train_acc': '74.319997'}]
[Epoch:0060/0600 lr:0.022542]: 100%|██| 521/521 [00:37<00:00, 13.93it/s, log={'train_loss': '0.727671', 'train_acc': '74.103997'}]
[Val_Accuracy epoch:60] val_loss:0.776822, val_acc:72.709997
[Epoch:0061/0600 lr:0.022500]: 100%|██| 521/521 [00:37<00:00, 13.83it/s, log={'train_loss': '0.718689', 'train_acc': '74.735997'}]
[Epoch:0062/0600 lr:0.022458]: 100%|██| 521/521 [00:37<00:00, 13.96it/s, log={'train_loss': '0.708785', 'train_acc': '74.733997'}]
[Epoch:0063/0600 lr:0.022417]: 100%|██| 521/521 [00:35<00:00, 14.50it/s, log={'train_loss': '0.702052', 'train_acc': '75.301997'}]
[Epoch:0064/0600 lr:0.022375]: 100%|██| 521/521 [00:36<00:00, 14.41it/s, log={'train_loss': '0.698668', 'train_acc': '75.405998'}]
[Epoch:0065/0600 lr:0.022333]: 100%|██| 521/521 [00:37<00:00, 14.02it/s, log={'train_loss': '0.687589', 'train_acc': '75.599997'}]
[Val_Accuracy epoch:65] val_loss:0.710857, val_acc:75.259998
[Epoch:0066/0600 lr:0.022292]: 100%|██| 521/521 [00:38<00:00, 13.68it/s, log={'train_loss': '0.686485', 'train_acc': '75.647997'}]
[Epoch:0067/0600 lr:0.022250]: 100%|██| 521/521 [00:36<00:00, 14.30it/s, log={'train_loss': '0.676068', 'train_acc': '76.241997'}]
[Epoch:0068/0600 lr:0.022208]: 100%|██| 521/521 [00:36<00:00, 14.13it/s, log={'train_loss': '0.668855', 'train_acc': '76.257997'}]
[Epoch:0069/0600 lr:0.022167]: 100%|██| 521/521 [00:37<00:00, 14.05it/s, log={'train_loss': '0.666953', 'train_acc': '76.415997'}]
[Epoch:0070/0600 lr:0.022125]: 100%|██| 521/521 [00:37<00:00, 13.81it/s, log={'train_loss': '0.657802', 'train_acc': '76.687997'}]
[Val_Accuracy epoch:70] val_loss:0.727286, val_acc:74.609998
[Epoch:0071/0600 lr:0.022083]: 100%|██| 521/521 [00:37<00:00, 13.79it/s, log={'train_loss': '0.651728', 'train_acc': '77.029997'}]
[Epoch:0072/0600 lr:0.022042]: 100%|██| 521/521 [00:37<00:00, 13.77it/s, log={'train_loss': '0.649371', 'train_acc': '77.077998'}]
[Epoch:0073/0600 lr:0.022000]: 100%|██| 521/521 [00:37<00:00, 13.73it/s, log={'train_loss': '0.643199', 'train_acc': '77.345997'}]
[Epoch:0074/0600 lr:0.021958]: 100%|██| 521/521 [00:37<00:00, 13.97it/s, log={'train_loss': '0.635533', 'train_acc': '77.427997'}]
[Epoch:0075/0600 lr:0.021917]: 100%|██| 521/521 [00:38<00:00, 13.68it/s, log={'train_loss': '0.630521', 'train_acc': '77.993997'}]
[Val_Accuracy epoch:75] val_loss:0.666485, val_acc:77.269997
[Epoch:0076/0600 lr:0.021875]: 100%|██| 521/521 [00:37<00:00, 13.87it/s, log={'train_loss': '0.626055', 'train_acc': '77.915998'}]
[Epoch:0077/0600 lr:0.021833]: 100%|██| 521/521 [00:35<00:00, 14.56it/s, log={'train_loss': '0.621241', 'train_acc': '78.067998'}]
[Epoch:0078/0600 lr:0.021792]: 100%|██| 521/521 [00:37<00:00, 13.82it/s, log={'train_loss': '0.611968', 'train_acc': '78.515997'}]
[Epoch:0079/0600 lr:0.021750]: 100%|██| 521/521 [00:35<00:00, 14.61it/s, log={'train_loss': '0.607348', 'train_acc': '78.495997'}]
[Epoch:0080/0600 lr:0.021708]: 100%|██| 521/521 [00:37<00:00, 13.88it/s, log={'train_loss': '0.601948', 'train_acc': '78.929997'}]
[Val_Accuracy epoch:80] val_loss:0.657434, val_acc:76.819998
[Epoch:0081/0600 lr:0.021667]: 100%|██| 521/521 [00:38<00:00, 13.60it/s, log={'train_loss': '0.593840', 'train_acc': '78.859997'}]
[Epoch:0082/0600 lr:0.021625]: 100%|██| 521/521 [00:37<00:00, 13.86it/s, log={'train_loss': '0.591795', 'train_acc': '79.123998'}]
[Epoch:0083/0600 lr:0.021583]: 100%|██| 521/521 [00:35<00:00, 14.63it/s, log={'train_loss': '0.586718', 'train_acc': '79.375997'}]
[Epoch:0084/0600 lr:0.021542]: 100%|██| 521/521 [00:37<00:00, 13.92it/s, log={'train_loss': '0.578337', 'train_acc': '79.537998'}]
[Epoch:0085/0600 lr:0.021500]: 100%|██| 521/521 [00:36<00:00, 14.33it/s, log={'train_loss': '0.573974', 'train_acc': '79.829997'}]
[Val_Accuracy epoch:85] val_loss:0.643145, val_acc:77.769998
[Epoch:0086/0600 lr:0.021458]: 100%|██| 521/521 [00:36<00:00, 14.12it/s, log={'train_loss': '0.571196', 'train_acc': '79.817997'}]
[Epoch:0087/0600 lr:0.021417]: 100%|██| 521/521 [00:37<00:00, 13.73it/s, log={'train_loss': '0.566516', 'train_acc': '80.061997'}]
[Epoch:0088/0600 lr:0.021375]: 100%|██| 521/521 [00:35<00:00, 14.70it/s, log={'train_loss': '0.563683', 'train_acc': '80.351997'}]
[Epoch:0089/0600 lr:0.021333]: 100%|██| 521/521 [00:35<00:00, 14.55it/s, log={'train_loss': '0.556440', 'train_acc': '80.333997'}]
[Epoch:0090/0600 lr:0.021292]: 100%|██| 521/521 [00:37<00:00, 13.94it/s, log={'train_loss': '0.553735', 'train_acc': '80.545997'}]
[Val_Accuracy epoch:90] val_loss:0.641132, val_acc:77.329997
[Epoch:0091/0600 lr:0.021250]: 100%|██| 521/521 [00:38<00:00, 13.70it/s, log={'train_loss': '0.546719', 'train_acc': '80.965997'}]
[Epoch:0092/0600 lr:0.021208]: 100%|██| 521/521 [00:37<00:00, 13.72it/s, log={'train_loss': '0.545965', 'train_acc': '80.759997'}]
[Epoch:0093/0600 lr:0.021167]: 100%|██| 521/521 [00:37<00:00, 13.99it/s, log={'train_loss': '0.534038', 'train_acc': '81.335997'}]
[Epoch:0094/0600 lr:0.021125]: 100%|██| 521/521 [00:36<00:00, 14.26it/s, log={'train_loss': '0.535085', 'train_acc': '81.221997'}]
[Epoch:0095/0600 lr:0.021083]: 100%|██| 521/521 [00:36<00:00, 14.26it/s, log={'train_loss': '0.533661', 'train_acc': '81.159998'}]
[Val_Accuracy epoch:95] val_loss:0.604470, val_acc:79.329997
[Epoch:0096/0600 lr:0.021042]: 100%|██| 521/521 [00:37<00:00, 13.99it/s, log={'train_loss': '0.527149', 'train_acc': '81.497998'}]
[Epoch:0097/0600 lr:0.021000]: 100%|██| 521/521 [00:36<00:00, 14.22it/s, log={'train_loss': '0.520440', 'train_acc': '81.617997'}]
[Epoch:0098/0600 lr:0.020958]: 100%|██| 521/521 [00:38<00:00, 13.67it/s, log={'train_loss': '0.519854', 'train_acc': '81.707997'}]
[Epoch:0099/0600 lr:0.020917]: 100%|██| 521/521 [00:35<00:00, 14.63it/s, log={'train_loss': '0.515520', 'train_acc': '81.855997'}]
[Epoch:0100/0600 lr:0.020875]: 100%|██| 521/521 [00:37<00:00, 13.97it/s, log={'train_loss': '0.508638', 'train_acc': '82.133997'}]
[Val_Accuracy epoch:100] val_loss:0.586721, val_acc:79.839997
[Epoch:0101/0600 lr:0.020833]: 100%|██| 521/521 [00:37<00:00, 13.81it/s, log={'train_loss': '0.506650', 'train_acc': '82.265997'}]
[Epoch:0102/0600 lr:0.020792]: 100%|██| 521/521 [00:35<00:00, 14.61it/s, log={'train_loss': '0.497311', 'train_acc': '82.541997'}]
[Epoch:0103/0600 lr:0.020750]: 100%|██| 521/521 [00:37<00:00, 13.94it/s, log={'train_loss': '0.493775', 'train_acc': '82.699997'}]
[Epoch:0104/0600 lr:0.020708]: 100%|██| 521/521 [00:37<00:00, 13.83it/s, log={'train_loss': '0.493742', 'train_acc': '82.731997'}]
[Epoch:0105/0600 lr:0.020667]: 100%|██| 521/521 [00:37<00:00, 14.05it/s, log={'train_loss': '0.488135', 'train_acc': '82.889997'}]
[Val_Accuracy epoch:105] val_loss:0.559219, val_acc:80.749998
[Epoch:0106/0600 lr:0.020625]: 100%|██| 521/521 [00:36<00:00, 14.25it/s, log={'train_loss': '0.490365', 'train_acc': '82.807997'}]
[Epoch:0107/0600 lr:0.020583]: 100%|██| 521/521 [00:36<00:00, 14.47it/s, log={'train_loss': '0.484605', 'train_acc': '82.927998'}]
[Epoch:0108/0600 lr:0.020542]: 100%|██| 521/521 [00:35<00:00, 14.48it/s, log={'train_loss': '0.476556', 'train_acc': '83.127998'}]
[Epoch:0109/0600 lr:0.020500]: 100%|██| 521/521 [00:35<00:00, 14.54it/s, log={'train_loss': '0.476323', 'train_acc': '83.231997'}]
[Epoch:0110/0600 lr:0.020458]: 100%|██| 521/521 [00:36<00:00, 14.25it/s, log={'train_loss': '0.471428', 'train_acc': '83.263997'}]
[Val_Accuracy epoch:110] val_loss:0.562173, val_acc:80.889997
[Epoch:0111/0600 lr:0.020417]: 100%|██| 521/521 [00:37<00:00, 13.92it/s, log={'train_loss': '0.467882', 'train_acc': '83.559997'}]
[Epoch:0112/0600 lr:0.020375]: 100%|██| 521/521 [00:35<00:00, 14.70it/s, log={'train_loss': '0.468115', 'train_acc': '83.641997'}]
[Epoch:0113/0600 lr:0.020333]: 100%|██| 521/521 [00:37<00:00, 13.81it/s, log={'train_loss': '0.456025', 'train_acc': '83.955997'}]
[Epoch:0114/0600 lr:0.020292]: 100%|██| 521/521 [00:37<00:00, 13.87it/s, log={'train_loss': '0.456705', 'train_acc': '83.869997'}]
[Epoch:0115/0600 lr:0.020250]: 100%|██| 521/521 [00:35<00:00, 14.49it/s, log={'train_loss': '0.454078', 'train_acc': '83.869998'}]
[Val_Accuracy epoch:115] val_loss:0.557729, val_acc:80.529998
[Epoch:0116/0600 lr:0.020208]: 100%|██| 521/521 [00:37<00:00, 13.75it/s, log={'train_loss': '0.448381', 'train_acc': '84.181997'}]
[Epoch:0117/0600 lr:0.020167]: 100%|██| 521/521 [00:37<00:00, 13.75it/s, log={'train_loss': '0.446341', 'train_acc': '84.219998'}]
[Epoch:0118/0600 lr:0.020125]: 100%|██| 521/521 [00:37<00:00, 13.79it/s, log={'train_loss': '0.443779', 'train_acc': '84.223998'}]
[Epoch:0119/0600 lr:0.020083]: 100%|██| 521/521 [00:37<00:00, 13.82it/s, log={'train_loss': '0.440935', 'train_acc': '84.455998'}]
[Epoch:0120/0600 lr:0.020042]: 100%|██| 521/521 [00:35<00:00, 14.60it/s, log={'train_loss': '0.441361', 'train_acc': '84.529997'}]
[Val_Accuracy epoch:120] val_loss:0.550475, val_acc:81.039997
[Epoch:0121/0600 lr:0.020000]: 100%|██| 521/521 [00:36<00:00, 14.25it/s, log={'train_loss': '0.438493', 'train_acc': '84.549997'}]
[Epoch:0122/0600 lr:0.019958]: 100%|██| 521/521 [00:36<00:00, 14.45it/s, log={'train_loss': '0.428737', 'train_acc': '85.063997'}]
[Epoch:0123/0600 lr:0.019917]: 100%|██| 521/521 [00:36<00:00, 14.31it/s, log={'train_loss': '0.425398', 'train_acc': '85.057998'}]
[Epoch:0124/0600 lr:0.019875]: 100%|██| 521/521 [00:37<00:00, 13.90it/s, log={'train_loss': '0.431733', 'train_acc': '84.889998'}]
[Epoch:0125/0600 lr:0.019833]: 100%|██| 521/521 [00:37<00:00, 14.02it/s, log={'train_loss': '0.426486', 'train_acc': '84.973997'}]
[Val_Accuracy epoch:125] val_loss:0.539201, val_acc:81.859997
[Epoch:0126/0600 lr:0.019792]: 100%|██| 521/521 [00:37<00:00, 14.02it/s, log={'train_loss': '0.421637', 'train_acc': '85.211997'}]
[Epoch:0127/0600 lr:0.019750]: 100%|██| 521/521 [00:36<00:00, 14.13it/s, log={'train_loss': '0.415735', 'train_acc': '85.377998'}]
[Epoch:0128/0600 lr:0.019708]: 100%|██| 521/521 [00:38<00:00, 13.70it/s, log={'train_loss': '0.416108', 'train_acc': '85.213998'}]
[Epoch:0129/0600 lr:0.019667]: 100%|██| 521/521 [00:36<00:00, 14.27it/s, log={'train_loss': '0.412779', 'train_acc': '85.497997'}]
[Epoch:0130/0600 lr:0.019625]: 100%|██| 521/521 [00:37<00:00, 13.96it/s, log={'train_loss': '0.410320', 'train_acc': '85.525998'}]
[Val_Accuracy epoch:130] val_loss:0.514068, val_acc:82.469997
[Epoch:0131/0600 lr:0.019583]: 100%|██| 521/521 [00:37<00:00, 13.95it/s, log={'train_loss': '0.406964', 'train_acc': '85.685997'}]
[Epoch:0132/0600 lr:0.019542]: 100%|██| 521/521 [00:35<00:00, 14.61it/s, log={'train_loss': '0.405117', 'train_acc': '85.703997'}]
[Epoch:0133/0600 lr:0.019500]: 100%|██| 521/521 [00:36<00:00, 14.42it/s, log={'train_loss': '0.401422', 'train_acc': '85.865998'}]
[Epoch:0134/0600 lr:0.019458]: 100%|██| 521/521 [00:36<00:00, 14.42it/s, log={'train_loss': '0.394637', 'train_acc': '86.179997'}]
[Epoch:0135/0600 lr:0.019417]: 100%|██| 521/521 [00:38<00:00, 13.57it/s, log={'train_loss': '0.397276', 'train_acc': '86.039997'}]
[Val_Accuracy epoch:135] val_loss:0.522806, val_acc:82.829998
[Epoch:0136/0600 lr:0.019375]: 100%|██| 521/521 [00:38<00:00, 13.44it/s, log={'train_loss': '0.393420', 'train_acc': '86.037997'}]
[Epoch:0137/0600 lr:0.019333]: 100%|██| 521/521 [00:35<00:00, 14.61it/s, log={'train_loss': '0.388715', 'train_acc': '86.265998'}]
[Epoch:0138/0600 lr:0.019292]: 100%|██| 521/521 [00:36<00:00, 14.11it/s, log={'train_loss': '0.392461', 'train_acc': '86.033997'}]
[Epoch:0139/0600 lr:0.019250]: 100%|██| 521/521 [00:36<00:00, 14.14it/s, log={'train_loss': '0.382374', 'train_acc': '86.509998'}]
[Epoch:0140/0600 lr:0.019208]: 100%|██| 521/521 [00:38<00:00, 13.64it/s, log={'train_loss': '0.385193', 'train_acc': '86.403997'}]
[Val_Accuracy epoch:140] val_loss:0.549702, val_acc:81.859998
[Epoch:0141/0600 lr:0.019167]: 100%|██| 521/521 [00:35<00:00, 14.67it/s, log={'train_loss': '0.383460', 'train_acc': '86.399997'}]
[Epoch:0142/0600 lr:0.019125]: 100%|██| 521/521 [00:37<00:00, 13.89it/s, log={'train_loss': '0.379245', 'train_acc': '86.601998'}]
[Epoch:0143/0600 lr:0.019083]: 100%|██| 521/521 [00:37<00:00, 14.00it/s, log={'train_loss': '0.378132', 'train_acc': '86.679998'}]
[Epoch:0144/0600 lr:0.019042]: 100%|██| 521/521 [00:37<00:00, 14.03it/s, log={'train_loss': '0.372227', 'train_acc': '86.823998'}]
[Epoch:0145/0600 lr:0.019000]: 100%|██| 521/521 [00:37<00:00, 14.01it/s, log={'train_loss': '0.368019', 'train_acc': '86.789997'}]
[Val_Accuracy epoch:145] val_loss:0.489915, val_acc:84.029997
[Epoch:0146/0600 lr:0.018958]: 100%|██| 521/521 [00:37<00:00, 13.90it/s, log={'train_loss': '0.367702', 'train_acc': '87.047998'}]
[Epoch:0147/0600 lr:0.018917]: 100%|██| 521/521 [00:36<00:00, 14.33it/s, log={'train_loss': '0.363315', 'train_acc': '87.125997'}]
[Epoch:0148/0600 lr:0.018875]: 100%|██| 521/521 [00:38<00:00, 13.70it/s, log={'train_loss': '0.368675', 'train_acc': '86.871998'}]
[Epoch:0149/0600 lr:0.018833]: 100%|██| 521/521 [00:38<00:00, 13.70it/s, log={'train_loss': '0.366366', 'train_acc': '86.959998'}]
[Epoch:0150/0600 lr:0.018792]: 100%|██| 521/521 [00:37<00:00, 13.91it/s, log={'train_loss': '0.360347', 'train_acc': '87.093998'}]
[Val_Accuracy epoch:150] val_loss:0.494743, val_acc:83.569998
[Epoch:0151/0600 lr:0.018750]: 100%|██| 521/521 [00:35<00:00, 14.47it/s, log={'train_loss': '0.358023', 'train_acc': '87.297998'}]
[Epoch:0152/0600 lr:0.018708]: 100%|██| 521/521 [00:35<00:00, 14.71it/s, log={'train_loss': '0.356801', 'train_acc': '87.235998'}]
[Epoch:0153/0600 lr:0.018667]: 100%|██| 521/521 [00:35<00:00, 14.59it/s, log={'train_loss': '0.355924', 'train_acc': '87.465997'}]
[Epoch:0154/0600 lr:0.018625]: 100%|██| 521/521 [00:34<00:00, 15.02it/s, log={'train_loss': '0.355075', 'train_acc': '87.467997'}]
[Epoch:0155/0600 lr:0.018583]: 100%|██| 521/521 [00:36<00:00, 14.43it/s, log={'train_loss': '0.351686', 'train_acc': '87.539998'}]
[Val_Accuracy epoch:155] val_loss:0.516895, val_acc:82.539997
[Epoch:0156/0600 lr:0.018542]: 100%|██| 521/521 [00:37<00:00, 13.97it/s, log={'train_loss': '0.346697', 'train_acc': '87.885997'}]
[Epoch:0157/0600 lr:0.018500]: 100%|██| 521/521 [00:36<00:00, 14.17it/s, log={'train_loss': '0.345881', 'train_acc': '87.755997'}]
[Epoch:0158/0600 lr:0.018458]: 100%|██| 521/521 [00:37<00:00, 13.73it/s, log={'train_loss': '0.347353', 'train_acc': '87.651997'}]
[Epoch:0159/0600 lr:0.018417]: 100%|██| 521/521 [00:37<00:00, 13.97it/s, log={'train_loss': '0.341228', 'train_acc': '87.865997'}]
[Epoch:0160/0600 lr:0.018375]: 100%|██| 521/521 [00:37<00:00, 13.76it/s, log={'train_loss': '0.338105', 'train_acc': '88.027998'}]
[Val_Accuracy epoch:160] val_loss:0.465242, val_acc:84.549997
[Epoch:0161/0600 lr:0.018333]: 100%|██| 521/521 [00:37<00:00, 13.72it/s, log={'train_loss': '0.340163', 'train_acc': '88.039997'}]
[Epoch:0162/0600 lr:0.018292]: 100%|██| 521/521 [00:35<00:00, 14.55it/s, log={'train_loss': '0.334136', 'train_acc': '88.207998'}]
[Epoch:0163/0600 lr:0.018250]: 100%|██| 521/521 [00:37<00:00, 13.86it/s, log={'train_loss': '0.335396', 'train_acc': '88.029997'}]
[Epoch:0164/0600 lr:0.018208]: 100%|██| 521/521 [00:36<00:00, 14.45it/s, log={'train_loss': '0.332718', 'train_acc': '88.127997'}]
[Epoch:0165/0600 lr:0.018167]: 100%|██| 521/521 [00:36<00:00, 14.22it/s, log={'train_loss': '0.326152', 'train_acc': '88.439998'}]
[Val_Accuracy epoch:165] val_loss:0.490521, val_acc:84.299998
[Epoch:0166/0600 lr:0.018125]: 100%|██| 521/521 [00:38<00:00, 13.57it/s, log={'train_loss': '0.328003', 'train_acc': '88.341997'}]
[Epoch:0167/0600 lr:0.018083]: 100%|██| 521/521 [00:35<00:00, 14.49it/s, log={'train_loss': '0.326219', 'train_acc': '88.537997'}]
[Epoch:0168/0600 lr:0.018042]: 100%|██| 521/521 [00:37<00:00, 13.77it/s, log={'train_loss': '0.323045', 'train_acc': '88.581997'}]
[Epoch:0169/0600 lr:0.018000]: 100%|██| 521/521 [00:38<00:00, 13.70it/s, log={'train_loss': '0.323518', 'train_acc': '88.501997'}]
[Epoch:0170/0600 lr:0.017958]: 100%|██| 521/521 [00:35<00:00, 14.51it/s, log={'train_loss': '0.320845', 'train_acc': '88.703997'}]
[Val_Accuracy epoch:170] val_loss:0.456162, val_acc:85.069997
[Epoch:0171/0600 lr:0.017917]: 100%|██| 521/521 [00:37<00:00, 13.74it/s, log={'train_loss': '0.315444', 'train_acc': '88.729997'}]
[Epoch:0172/0600 lr:0.017875]: 100%|██| 521/521 [00:35<00:00, 14.66it/s, log={'train_loss': '0.315788', 'train_acc': '88.795998'}]
[Epoch:0173/0600 lr:0.017833]: 100%|██| 521/521 [00:37<00:00, 13.98it/s, log={'train_loss': '0.314792', 'train_acc': '88.733998'}]
[Epoch:0174/0600 lr:0.017792]: 100%|██| 521/521 [00:37<00:00, 13.83it/s, log={'train_loss': '0.315338', 'train_acc': '88.751997'}]
[Epoch:0175/0600 lr:0.017750]: 100%|██| 521/521 [00:36<00:00, 14.45it/s, log={'train_loss': '0.314188', 'train_acc': '88.767997'}]
[Val_Accuracy epoch:175] val_loss:0.490349, val_acc:83.589998
[Epoch:0176/0600 lr:0.017708]: 100%|██| 521/521 [00:37<00:00, 13.77it/s, log={'train_loss': '0.305918', 'train_acc': '89.169998'}]
[Epoch:0177/0600 lr:0.017667]: 100%|██| 521/521 [00:36<00:00, 14.21it/s, log={'train_loss': '0.310380', 'train_acc': '88.951997'}]
[Epoch:0178/0600 lr:0.017625]: 100%|██| 521/521 [00:37<00:00, 14.05it/s, log={'train_loss': '0.305265', 'train_acc': '89.225998'}]
[Epoch:0179/0600 lr:0.017583]: 100%|██| 521/521 [00:37<00:00, 14.08it/s, log={'train_loss': '0.305736', 'train_acc': '89.069998'}]
[Epoch:0180/0600 lr:0.017542]: 100%|██| 521/521 [00:37<00:00, 13.82it/s, log={'train_loss': '0.301395', 'train_acc': '89.139998'}]
[Val_Accuracy epoch:180] val_loss:0.473987, val_acc:84.499998
[Epoch:0181/0600 lr:0.017500]: 100%|██| 521/521 [00:37<00:00, 13.99it/s, log={'train_loss': '0.301866', 'train_acc': '89.207998'}]
[Epoch:0182/0600 lr:0.017458]: 100%|██| 521/521 [00:36<00:00, 14.22it/s, log={'train_loss': '0.301491', 'train_acc': '89.317997'}]
[Epoch:0183/0600 lr:0.017417]: 100%|██| 521/521 [00:36<00:00, 14.26it/s, log={'train_loss': '0.300855', 'train_acc': '89.239997'}]
[Epoch:0184/0600 lr:0.017375]: 100%|██| 521/521 [00:38<00:00, 13.64it/s, log={'train_loss': '0.291677', 'train_acc': '89.567997'}]
[Epoch:0185/0600 lr:0.017333]: 100%|██| 521/521 [00:37<00:00, 14.05it/s, log={'train_loss': '0.296283', 'train_acc': '89.373997'}]
[Val_Accuracy epoch:185] val_loss:0.453395, val_acc:85.119998
[Epoch:0186/0600 lr:0.017292]: 100%|██| 521/521 [00:36<00:00, 14.30it/s, log={'train_loss': '0.289943', 'train_acc': '89.693998'}]
[Epoch:0187/0600 lr:0.017250]: 100%|██| 521/521 [00:37<00:00, 13.79it/s, log={'train_loss': '0.290366', 'train_acc': '89.731997'}]
[Epoch:0188/0600 lr:0.017208]: 100%|██| 521/521 [00:35<00:00, 14.62it/s, log={'train_loss': '0.285477', 'train_acc': '89.809997'}]
[Epoch:0189/0600 lr:0.017167]: 100%|██| 521/521 [00:37<00:00, 13.73it/s, log={'train_loss': '0.285774', 'train_acc': '89.757997'}]
[Epoch:0190/0600 lr:0.017125]: 100%|██| 521/521 [00:36<00:00, 14.42it/s, log={'train_loss': '0.287618', 'train_acc': '89.633997'}]
[Val_Accuracy epoch:190] val_loss:0.468825, val_acc:85.219997
[Epoch:0191/0600 lr:0.017083]: 100%|██| 521/521 [00:37<00:00, 13.90it/s, log={'train_loss': '0.285295', 'train_acc': '89.837998'}]
[Epoch:0192/0600 lr:0.017042]: 100%|██| 521/521 [00:36<00:00, 14.45it/s, log={'train_loss': '0.285521', 'train_acc': '89.837997'}]
[Epoch:0193/0600 lr:0.017000]: 100%|██| 521/521 [00:35<00:00, 14.56it/s, log={'train_loss': '0.285651', 'train_acc': '89.743998'}]
[Epoch:0194/0600 lr:0.016958]: 100%|██| 521/521 [00:36<00:00, 14.22it/s, log={'train_loss': '0.278801', 'train_acc': '90.099998'}]
[Epoch:0195/0600 lr:0.016917]: 100%|██| 521/521 [00:36<00:00, 14.32it/s, log={'train_loss': '0.279747', 'train_acc': '90.051998'}]
[Val_Accuracy epoch:195] val_loss:0.456070, val_acc:85.129998
[Epoch:0196/0600 lr:0.016875]: 100%|██| 521/521 [00:36<00:00, 14.25it/s, log={'train_loss': '0.276892', 'train_acc': '90.191997'}]
[Epoch:0197/0600 lr:0.016833]: 100%|██| 521/521 [00:37<00:00, 14.02it/s, log={'train_loss': '0.271483', 'train_acc': '90.263997'}]
[Epoch:0198/0600 lr:0.016792]: 100%|██| 521/521 [00:36<00:00, 14.34it/s, log={'train_loss': '0.271599', 'train_acc': '90.333997'}]
[Epoch:0199/0600 lr:0.016750]: 100%|██| 521/521 [00:36<00:00, 14.31it/s, log={'train_loss': '0.271427', 'train_acc': '90.291997'}]
[Epoch:0200/0600 lr:0.016708]: 100%|██| 521/521 [00:37<00:00, 14.03it/s, log={'train_loss': '0.274910', 'train_acc': '90.181997'}]
[Val_Accuracy epoch:200] val_loss:0.456841, val_acc:85.539997
[Epoch:0201/0600 lr:0.016667]: 100%|██| 521/521 [00:36<00:00, 14.26it/s, log={'train_loss': '0.273767', 'train_acc': '90.155998'}]
[Epoch:0202/0600 lr:0.016625]: 100%|██| 521/521 [00:37<00:00, 13.97it/s, log={'train_loss': '0.266612', 'train_acc': '90.485998'}]
[Epoch:0203/0600 lr:0.016583]: 100%|██| 521/521 [00:37<00:00, 13.89it/s, log={'train_loss': '0.264738', 'train_acc': '90.501997'}]
[Epoch:0204/0600 lr:0.016542]: 100%|██| 521/521 [00:36<00:00, 14.26it/s, log={'train_loss': '0.261558', 'train_acc': '90.647998'}]
[Epoch:0205/0600 lr:0.016500]: 100%|██| 521/521 [00:36<00:00, 14.27it/s, log={'train_loss': '0.263270', 'train_acc': '90.527997'}]
[Val_Accuracy epoch:205] val_loss:0.453599, val_acc:85.829998
[Epoch:0206/0600 lr:0.016458]: 100%|██| 521/521 [00:36<00:00, 14.24it/s, log={'train_loss': '0.264965', 'train_acc': '90.559997'}]
[Epoch:0207/0600 lr:0.016417]: 100%|██| 521/521 [00:36<00:00, 14.25it/s, log={'train_loss': '0.258027', 'train_acc': '90.759997'}]
[Epoch:0208/0600 lr:0.016375]: 100%|██| 521/521 [00:36<00:00, 14.39it/s, log={'train_loss': '0.258534', 'train_acc': '90.719998'}]
[Epoch:0209/0600 lr:0.016333]: 100%|██| 521/521 [00:35<00:00, 14.60it/s, log={'train_loss': '0.255735', 'train_acc': '90.815997'}]
[Epoch:0210/0600 lr:0.016292]: 100%|██| 521/521 [00:36<00:00, 14.17it/s, log={'train_loss': '0.258058', 'train_acc': '90.673997'}]
[Val_Accuracy epoch:210] val_loss:0.477699, val_acc:84.959997
[Epoch:0211/0600 lr:0.016250]: 100%|██| 521/521 [00:37<00:00, 13.92it/s, log={'train_loss': '0.256041', 'train_acc': '90.739997'}]
[Epoch:0212/0600 lr:0.016208]: 100%|██| 521/521 [00:35<00:00, 14.49it/s, log={'train_loss': '0.252881', 'train_acc': '90.829997'}]
[Epoch:0213/0600 lr:0.016167]: 100%|██| 521/521 [00:37<00:00, 14.07it/s, log={'train_loss': '0.256385', 'train_acc': '90.675998'}]
[Epoch:0214/0600 lr:0.016125]: 100%|██| 521/521 [00:38<00:00, 13.70it/s, log={'train_loss': '0.251605', 'train_acc': '91.005997'}]
[Epoch:0215/0600 lr:0.016083]: 100%|██| 521/521 [00:38<00:00, 13.65it/s, log={'train_loss': '0.247802', 'train_acc': '91.045997'}]
[Val_Accuracy epoch:215] val_loss:0.425162, val_acc:86.709998
[Epoch:0216/0600 lr:0.016042]: 100%|██| 521/521 [00:36<00:00, 14.10it/s, log={'train_loss': '0.246994', 'train_acc': '91.093998'}]
[Epoch:0217/0600 lr:0.016000]: 100%|██| 521/521 [00:37<00:00, 13.94it/s, log={'train_loss': '0.245129', 'train_acc': '91.189997'}]
[Epoch:0218/0600 lr:0.015958]: 100%|██| 521/521 [00:37<00:00, 13.83it/s, log={'train_loss': '0.243199', 'train_acc': '91.217997'}]
[Epoch:0219/0600 lr:0.015917]: 100%|██| 521/521 [00:36<00:00, 14.39it/s, log={'train_loss': '0.242311', 'train_acc': '91.243997'}]
[Epoch:0220/0600 lr:0.015875]: 100%|██| 521/521 [00:36<00:00, 14.26it/s, log={'train_loss': '0.240702', 'train_acc': '91.345997'}]
[Val_Accuracy epoch:220] val_loss:0.464249, val_acc:85.739997
[Epoch:0221/0600 lr:0.015833]: 100%|██| 521/521 [00:35<00:00, 14.70it/s, log={'train_loss': '0.240275', 'train_acc': '91.351997'}]
[Epoch:0222/0600 lr:0.015792]: 100%|██| 521/521 [00:37<00:00, 13.88it/s, log={'train_loss': '0.238042', 'train_acc': '91.525998'}]
[Epoch:0223/0600 lr:0.015750]: 100%|██| 521/521 [00:35<00:00, 14.51it/s, log={'train_loss': '0.240548', 'train_acc': '91.321997'}]
[Epoch:0224/0600 lr:0.015708]: 100%|██| 521/521 [00:38<00:00, 13.54it/s, log={'train_loss': '0.240622', 'train_acc': '91.359997'}]
[Epoch:0225/0600 lr:0.015667]: 100%|██| 521/521 [00:36<00:00, 14.21it/s, log={'train_loss': '0.236375', 'train_acc': '91.463998'}]
[Val_Accuracy epoch:225] val_loss:0.488863, val_acc:85.159997
[Epoch:0226/0600 lr:0.015625]: 100%|██| 521/521 [00:37<00:00, 14.07it/s, log={'train_loss': '0.232592', 'train_acc': '91.495997'}]
[Epoch:0227/0600 lr:0.015583]: 100%|██| 521/521 [00:37<00:00, 14.00it/s, log={'train_loss': '0.232663', 'train_acc': '91.477997'}]
[Epoch:0228/0600 lr:0.015542]: 100%|██| 521/521 [00:36<00:00, 14.26it/s, log={'train_loss': '0.234451', 'train_acc': '91.543998'}]
[Epoch:0229/0600 lr:0.015500]: 100%|██| 521/521 [00:37<00:00, 13.94it/s, log={'train_loss': '0.232617', 'train_acc': '91.725998'}]
[Epoch:0230/0600 lr:0.015458]: 100%|██| 521/521 [00:36<00:00, 14.41it/s, log={'train_loss': '0.227807', 'train_acc': '91.919997'}]
[Val_Accuracy epoch:230] val_loss:0.450652, val_acc:86.429997
[Epoch:0231/0600 lr:0.015417]: 100%|██| 521/521 [00:35<00:00, 14.79it/s, log={'train_loss': '0.230612', 'train_acc': '91.793997'}]
[Epoch:0232/0600 lr:0.015375]: 100%|██| 521/521 [00:35<00:00, 14.84it/s, log={'train_loss': '0.225716', 'train_acc': '91.827998'}]
[Epoch:0233/0600 lr:0.015333]: 100%|██| 521/521 [00:37<00:00, 14.04it/s, log={'train_loss': '0.226179', 'train_acc': '91.877997'}]
[Epoch:0234/0600 lr:0.015292]: 100%|██| 521/521 [00:37<00:00, 13.80it/s, log={'train_loss': '0.220878', 'train_acc': '91.991997'}]
[Epoch:0235/0600 lr:0.015250]: 100%|██| 521/521 [00:37<00:00, 13.92it/s, log={'train_loss': '0.220357', 'train_acc': '91.919998'}]
[Val_Accuracy epoch:235] val_loss:0.450116, val_acc:86.159997
[Epoch:0236/0600 lr:0.015208]: 100%|██| 521/521 [00:37<00:00, 13.78it/s, log={'train_loss': '0.222171', 'train_acc': '91.941997'}]
[Epoch:0237/0600 lr:0.015167]: 100%|██| 521/521 [00:35<00:00, 14.54it/s, log={'train_loss': '0.223168', 'train_acc': '91.973997'}]
[Epoch:0238/0600 lr:0.015125]: 100%|██| 521/521 [00:38<00:00, 13.69it/s, log={'train_loss': '0.221120', 'train_acc': '92.023998'}]
[Epoch:0239/0600 lr:0.015083]: 100%|██| 521/521 [00:36<00:00, 14.41it/s, log={'train_loss': '0.219650', 'train_acc': '92.083997'}]
[Epoch:0240/0600 lr:0.015042]: 100%|██| 521/521 [00:36<00:00, 14.42it/s, log={'train_loss': '0.216956', 'train_acc': '92.105998'}]
[Val_Accuracy epoch:240] val_loss:0.494036, val_acc:84.999997
[Epoch:0241/0600 lr:0.015000]: 100%|██| 521/521 [00:36<00:00, 14.18it/s, log={'train_loss': '0.215923', 'train_acc': '92.119997'}]
[Epoch:0242/0600 lr:0.014958]: 100%|██| 521/521 [00:36<00:00, 14.09it/s, log={'train_loss': '0.213771', 'train_acc': '92.403998'}]
[Epoch:0243/0600 lr:0.014917]: 100%|██| 521/521 [00:35<00:00, 14.58it/s, log={'train_loss': '0.211523', 'train_acc': '92.311997'}]
[Epoch:0244/0600 lr:0.014875]: 100%|██| 521/521 [00:36<00:00, 14.11it/s, log={'train_loss': '0.210595', 'train_acc': '92.459998'}]
[Epoch:0245/0600 lr:0.014833]: 100%|██| 521/521 [00:36<00:00, 14.10it/s, log={'train_loss': '0.209731', 'train_acc': '92.503998'}]
[Val_Accuracy epoch:245] val_loss:0.463862, val_acc:85.879997
[Epoch:0246/0600 lr:0.014792]: 100%|██| 521/521 [00:37<00:00, 13.80it/s, log={'train_loss': '0.208869', 'train_acc': '92.447997'}]
[Epoch:0247/0600 lr:0.014750]: 100%|██| 521/521 [00:37<00:00, 13.78it/s, log={'train_loss': '0.209151', 'train_acc': '92.465998'}]
[Epoch:0248/0600 lr:0.014708]: 100%|██| 521/521 [00:37<00:00, 14.05it/s, log={'train_loss': '0.207910', 'train_acc': '92.479998'}]
[Epoch:0249/0600 lr:0.014667]: 100%|██| 521/521 [00:35<00:00, 14.62it/s, log={'train_loss': '0.207015', 'train_acc': '92.493997'}]
[Epoch:0250/0600 lr:0.014625]: 100%|██| 521/521 [00:35<00:00, 14.62it/s, log={'train_loss': '0.208892', 'train_acc': '92.447997'}]
[Val_Accuracy epoch:250] val_loss:0.465755, val_acc:86.609997
[Epoch:0251/0600 lr:0.014583]: 100%|██| 521/521 [00:37<00:00, 13.93it/s, log={'train_loss': '0.201156', 'train_acc': '92.667997'}]
[Epoch:0252/0600 lr:0.014542]: 100%|██| 521/521 [00:35<00:00, 14.53it/s, log={'train_loss': '0.200943', 'train_acc': '92.721998'}]
[Epoch:0253/0600 lr:0.014500]: 100%|██| 521/521 [00:35<00:00, 14.53it/s, log={'train_loss': '0.200540', 'train_acc': '92.751997'}]
[Epoch:0254/0600 lr:0.014458]: 100%|██| 521/521 [00:38<00:00, 13.65it/s, log={'train_loss': '0.201576', 'train_acc': '92.747997'}]
[Epoch:0255/0600 lr:0.014417]: 100%|██| 521/521 [00:35<00:00, 14.74it/s, log={'train_loss': '0.202060', 'train_acc': '92.643997'}]
[Val_Accuracy epoch:255] val_loss:0.450853, val_acc:86.259998
[Epoch:0256/0600 lr:0.014375]: 100%|██| 521/521 [00:36<00:00, 14.08it/s, log={'train_loss': '0.201749', 'train_acc': '92.681997'}]
[Epoch:0257/0600 lr:0.014333]: 100%|██| 521/521 [00:37<00:00, 13.74it/s, log={'train_loss': '0.200794', 'train_acc': '92.743997'}]
[Epoch:0258/0600 lr:0.014292]: 100%|██| 521/521 [00:35<00:00, 14.62it/s, log={'train_loss': '0.193536', 'train_acc': '92.971997'}]
[Epoch:0259/0600 lr:0.014250]: 100%|██| 521/521 [00:35<00:00, 14.54it/s, log={'train_loss': '0.199037', 'train_acc': '92.673997'}]
[Epoch:0260/0600 lr:0.014208]: 100%|██| 521/521 [00:36<00:00, 14.45it/s, log={'train_loss': '0.198454', 'train_acc': '92.763998'}]
[Val_Accuracy epoch:260] val_loss:0.477936, val_acc:86.349997
[Epoch:0261/0600 lr:0.014167]: 100%|██| 521/521 [00:35<00:00, 14.64it/s, log={'train_loss': '0.193787', 'train_acc': '92.949997'}]
[Epoch:0262/0600 lr:0.014125]: 100%|██| 521/521 [00:38<00:00, 13.64it/s, log={'train_loss': '0.193648', 'train_acc': '92.911997'}]
[Epoch:0263/0600 lr:0.014083]: 100%|██| 521/521 [00:37<00:00, 13.88it/s, log={'train_loss': '0.192885', 'train_acc': '93.097998'}]
[Epoch:0264/0600 lr:0.014042]: 100%|██| 521/521 [00:37<00:00, 13.86it/s, log={'train_loss': '0.193122', 'train_acc': '92.999997'}]
[Epoch:0265/0600 lr:0.014000]: 100%|██| 521/521 [00:37<00:00, 13.84it/s, log={'train_loss': '0.186398', 'train_acc': '93.191997'}]
[Val_Accuracy epoch:265] val_loss:0.453287, val_acc:86.919997
[Epoch:0266/0600 lr:0.013958]: 100%|██| 521/521 [00:37<00:00, 14.05it/s, log={'train_loss': '0.189720', 'train_acc': '93.173997'}]
[Epoch:0267/0600 lr:0.013917]: 100%|██| 521/521 [00:38<00:00, 13.61it/s, log={'train_loss': '0.183672', 'train_acc': '93.351997'}]
[Epoch:0268/0600 lr:0.013875]: 100%|██| 521/521 [00:37<00:00, 13.87it/s, log={'train_loss': '0.185123', 'train_acc': '93.297997'}]
[Epoch:0269/0600 lr:0.013833]: 100%|██| 521/521 [00:36<00:00, 14.16it/s, log={'train_loss': '0.186368', 'train_acc': '93.227997'}]
[Epoch:0270/0600 lr:0.013792]: 100%|██| 521/521 [00:36<00:00, 14.38it/s, log={'train_loss': '0.187011', 'train_acc': '93.321997'}]
[Val_Accuracy epoch:270] val_loss:0.491205, val_acc:86.379997
[Epoch:0271/0600 lr:0.013750]: 100%|██| 521/521 [00:37<00:00, 13.88it/s, log={'train_loss': '0.186145', 'train_acc': '93.347997'}]
[Epoch:0272/0600 lr:0.013708]: 100%|██| 521/521 [00:37<00:00, 14.01it/s, log={'train_loss': '0.179187', 'train_acc': '93.551997'}]
[Epoch:0273/0600 lr:0.013667]: 100%|██| 521/521 [00:38<00:00, 13.64it/s, log={'train_loss': '0.180514', 'train_acc': '93.367997'}]
[Epoch:0274/0600 lr:0.013625]: 100%|██| 521/521 [00:36<00:00, 14.42it/s, log={'train_loss': '0.180004', 'train_acc': '93.511998'}]
[Epoch:0275/0600 lr:0.013583]: 100%|██| 521/521 [00:36<00:00, 14.09it/s, log={'train_loss': '0.182740', 'train_acc': '93.337997'}]
[Val_Accuracy epoch:275] val_loss:0.455025, val_acc:86.859997
[Epoch:0276/0600 lr:0.013542]: 100%|██| 521/521 [00:37<00:00, 13.86it/s, log={'train_loss': '0.181106', 'train_acc': '93.483997'}]
[Epoch:0277/0600 lr:0.013500]: 100%|██| 521/521 [00:37<00:00, 13.82it/s, log={'train_loss': '0.174359', 'train_acc': '93.623998'}]
[Epoch:0278/0600 lr:0.013458]: 100%|██| 521/521 [00:38<00:00, 13.57it/s, log={'train_loss': '0.177501', 'train_acc': '93.485997'}]
[Epoch:0279/0600 lr:0.013417]: 100%|██| 521/521 [00:36<00:00, 14.25it/s, log={'train_loss': '0.174640', 'train_acc': '93.655997'}]
[Epoch:0280/0600 lr:0.013375]: 100%|██| 521/521 [00:35<00:00, 14.49it/s, log={'train_loss': '0.173718', 'train_acc': '93.725997'}]
[Val_Accuracy epoch:280] val_loss:0.445093, val_acc:87.499998
[Epoch:0281/0600 lr:0.013333]: 100%|██| 521/521 [00:36<00:00, 14.43it/s, log={'train_loss': '0.172652', 'train_acc': '93.685998'}]
[Epoch:0282/0600 lr:0.013292]: 100%|██| 521/521 [00:37<00:00, 13.99it/s, log={'train_loss': '0.176067', 'train_acc': '93.597998'}]
[Epoch:0283/0600 lr:0.013250]: 100%|██| 521/521 [00:36<00:00, 14.19it/s, log={'train_loss': '0.170773', 'train_acc': '93.899997'}]
[Epoch:0284/0600 lr:0.013208]: 100%|██| 521/521 [00:36<00:00, 14.42it/s, log={'train_loss': '0.171867', 'train_acc': '93.695998'}]
[Epoch:0285/0600 lr:0.013167]: 100%|██| 521/521 [00:36<00:00, 14.17it/s, log={'train_loss': '0.168863', 'train_acc': '93.943998'}]
[Val_Accuracy epoch:285] val_loss:0.502711, val_acc:86.359998
[Epoch:0286/0600 lr:0.013125]: 100%|██| 521/521 [00:37<00:00, 13.84it/s, log={'train_loss': '0.169947', 'train_acc': '93.827997'}]
[Epoch:0287/0600 lr:0.013083]: 100%|██| 521/521 [00:37<00:00, 13.80it/s, log={'train_loss': '0.165341', 'train_acc': '94.033998'}]
[Epoch:0288/0600 lr:0.013042]: 100%|██| 521/521 [00:37<00:00, 13.84it/s, log={'train_loss': '0.163038', 'train_acc': '94.153998'}]
[Epoch:0289/0600 lr:0.013000]: 100%|██| 521/521 [00:36<00:00, 14.12it/s, log={'train_loss': '0.165011', 'train_acc': '94.003997'}]
[Epoch:0290/0600 lr:0.012958]: 100%|██| 521/521 [00:37<00:00, 13.79it/s, log={'train_loss': '0.164373', 'train_acc': '93.957997'}]
[Val_Accuracy epoch:290] val_loss:0.474670, val_acc:87.199998
[Epoch:0291/0600 lr:0.012917]: 100%|██| 521/521 [00:36<00:00, 14.33it/s, log={'train_loss': '0.159167', 'train_acc': '94.295997'}]
[Epoch:0292/0600 lr:0.012875]: 100%|██| 521/521 [00:37<00:00, 13.90it/s, log={'train_loss': '0.165528', 'train_acc': '94.035997'}]
[Epoch:0293/0600 lr:0.012833]: 100%|██| 521/521 [00:36<00:00, 14.29it/s, log={'train_loss': '0.162433', 'train_acc': '94.077997'}]
[Epoch:0294/0600 lr:0.012792]: 100%|██| 521/521 [00:37<00:00, 13.86it/s, log={'train_loss': '0.162287', 'train_acc': '94.139997'}]
[Epoch:0295/0600 lr:0.012750]: 100%|██| 521/521 [00:37<00:00, 13.87it/s, log={'train_loss': '0.161339', 'train_acc': '94.145998'}]
[Val_Accuracy epoch:295] val_loss:0.463750, val_acc:86.979997
[Epoch:0296/0600 lr:0.012708]: 100%|██| 521/521 [00:38<00:00, 13.63it/s, log={'train_loss': '0.157984', 'train_acc': '94.137997'}]
[Epoch:0297/0600 lr:0.012667]: 100%|██| 521/521 [00:36<00:00, 14.13it/s, log={'train_loss': '0.157716', 'train_acc': '94.329997'}]
[Epoch:0298/0600 lr:0.012625]: 100%|██| 521/521 [00:35<00:00, 14.49it/s, log={'train_loss': '0.158450', 'train_acc': '94.287997'}]
[Epoch:0299/0600 lr:0.012583]: 100%|██| 521/521 [00:36<00:00, 14.36it/s, log={'train_loss': '0.155126', 'train_acc': '94.385998'}]
[Epoch:0300/0600 lr:0.012542]: 100%|██| 521/521 [00:37<00:00, 14.00it/s, log={'train_loss': '0.157067', 'train_acc': '94.311997'}]
[Val_Accuracy epoch:300] val_loss:0.511617, val_acc:85.979998
[Epoch:0301/0600 lr:0.012500]: 100%|██| 521/521 [00:36<00:00, 14.21it/s, log={'train_loss': '0.154863', 'train_acc': '94.379997'}]
[Epoch:0302/0600 lr:0.012458]: 100%|██| 521/521 [00:37<00:00, 13.78it/s, log={'train_loss': '0.155072', 'train_acc': '94.407997'}]
[Epoch:0303/0600 lr:0.012417]: 100%|██| 521/521 [00:37<00:00, 13.96it/s, log={'train_loss': '0.152661', 'train_acc': '94.431997'}]
[Epoch:0304/0600 lr:0.012375]: 100%|██| 521/521 [00:37<00:00, 13.81it/s, log={'train_loss': '0.151331', 'train_acc': '94.333997'}]
[Epoch:0305/0600 lr:0.012333]: 100%|██| 521/521 [00:35<00:00, 14.79it/s, log={'train_loss': '0.157059', 'train_acc': '94.271997'}]
[Val_Accuracy epoch:305] val_loss:0.481372, val_acc:86.759997
[Epoch:0306/0600 lr:0.012292]: 100%|██| 521/521 [00:36<00:00, 14.16it/s, log={'train_loss': '0.153833', 'train_acc': '94.417998'}]
[Epoch:0307/0600 lr:0.012250]: 100%|██| 521/521 [00:35<00:00, 14.52it/s, log={'train_loss': '0.149315', 'train_acc': '94.471998'}]
[Epoch:0308/0600 lr:0.012208]: 100%|██| 521/521 [00:36<00:00, 14.19it/s, log={'train_loss': '0.145531', 'train_acc': '94.617997'}]
[Epoch:0309/0600 lr:0.012167]: 100%|██| 521/521 [00:36<00:00, 14.20it/s, log={'train_loss': '0.147704', 'train_acc': '94.619997'}]
[Epoch:0310/0600 lr:0.012125]: 100%|██| 521/521 [00:35<00:00, 14.70it/s, log={'train_loss': '0.147554', 'train_acc': '94.549997'}]
[Val_Accuracy epoch:310] val_loss:0.457518, val_acc:87.749997
[Epoch:0311/0600 lr:0.012083]: 100%|██| 521/521 [00:36<00:00, 14.13it/s, log={'train_loss': '0.143491', 'train_acc': '94.711997'}]
[Epoch:0312/0600 lr:0.012042]: 100%|██| 521/521 [00:36<00:00, 14.38it/s, log={'train_loss': '0.147000', 'train_acc': '94.679998'}]
[Epoch:0313/0600 lr:0.012000]: 100%|██| 521/521 [00:37<00:00, 13.80it/s, log={'train_loss': '0.143933', 'train_acc': '94.773998'}]
[Epoch:0314/0600 lr:0.011958]: 100%|██| 521/521 [00:36<00:00, 14.47it/s, log={'train_loss': '0.144047', 'train_acc': '94.821997'}]
[Epoch:0315/0600 lr:0.011917]: 100%|██| 521/521 [00:36<00:00, 14.34it/s, log={'train_loss': '0.143386', 'train_acc': '94.861997'}]
[Val_Accuracy epoch:315] val_loss:0.463039, val_acc:86.899998
[Epoch:0316/0600 lr:0.011875]: 100%|██| 521/521 [00:38<00:00, 13.61it/s, log={'train_loss': '0.139916', 'train_acc': '94.961997'}]
[Epoch:0317/0600 lr:0.011833]: 100%|██| 521/521 [00:37<00:00, 13.86it/s, log={'train_loss': '0.140787', 'train_acc': '94.941998'}]
[Epoch:0318/0600 lr:0.011792]: 100%|██| 521/521 [00:37<00:00, 14.06it/s, log={'train_loss': '0.138315', 'train_acc': '95.017998'}]
[Epoch:0319/0600 lr:0.011750]: 100%|██| 521/521 [00:38<00:00, 13.59it/s, log={'train_loss': '0.139875', 'train_acc': '94.881997'}]
[Epoch:0320/0600 lr:0.011708]: 100%|██| 521/521 [00:37<00:00, 13.97it/s, log={'train_loss': '0.139562', 'train_acc': '94.913998'}]
[Val_Accuracy epoch:320] val_loss:0.480974, val_acc:86.359998
[Epoch:0321/0600 lr:0.011667]: 100%|██| 521/521 [00:37<00:00, 13.98it/s, log={'train_loss': '0.138751', 'train_acc': '94.885998'}]
[Epoch:0322/0600 lr:0.011625]: 100%|██| 521/521 [00:38<00:00, 13.69it/s, log={'train_loss': '0.139517', 'train_acc': '94.909997'}]
[Epoch:0323/0600 lr:0.011583]: 100%|██| 521/521 [00:36<00:00, 14.27it/s, log={'train_loss': '0.138159', 'train_acc': '95.049998'}]
[Epoch:0324/0600 lr:0.011542]: 100%|██| 521/521 [00:37<00:00, 14.04it/s, log={'train_loss': '0.135917', 'train_acc': '95.115997'}]
[Epoch:0325/0600 lr:0.011500]: 100%|██| 521/521 [00:37<00:00, 14.03it/s, log={'train_loss': '0.133156', 'train_acc': '95.203997'}]
[Val_Accuracy epoch:325] val_loss:0.464322, val_acc:87.319998
[Epoch:0326/0600 lr:0.011458]: 100%|██| 521/521 [00:37<00:00, 13.99it/s, log={'train_loss': '0.133428', 'train_acc': '95.067997'}]
[Epoch:0327/0600 lr:0.011417]: 100%|██| 521/521 [00:37<00:00, 13.81it/s, log={'train_loss': '0.133207', 'train_acc': '95.185998'}]
[Epoch:0328/0600 lr:0.011375]: 100%|██| 521/521 [00:37<00:00, 13.88it/s, log={'train_loss': '0.133965', 'train_acc': '95.183997'}]
[Epoch:0329/0600 lr:0.011333]: 100%|██| 521/521 [00:35<00:00, 14.70it/s, log={'train_loss': '0.131942', 'train_acc': '95.165998'}]
[Epoch:0330/0600 lr:0.011292]: 100%|██| 521/521 [00:35<00:00, 14.57it/s, log={'train_loss': '0.129450', 'train_acc': '95.173998'}]
[Val_Accuracy epoch:330] val_loss:0.461490, val_acc:87.379997
[Epoch:0331/0600 lr:0.011250]: 100%|██| 521/521 [00:37<00:00, 14.01it/s, log={'train_loss': '0.129957', 'train_acc': '95.191997'}]
[Epoch:0332/0600 lr:0.011208]: 100%|██| 521/521 [00:36<00:00, 14.47it/s, log={'train_loss': '0.131354', 'train_acc': '95.125997'}]
[Epoch:0333/0600 lr:0.011167]: 100%|██| 521/521 [00:37<00:00, 13.92it/s, log={'train_loss': '0.130881', 'train_acc': '95.325998'}]
[Epoch:0334/0600 lr:0.011125]: 100%|██| 521/521 [00:35<00:00, 14.48it/s, log={'train_loss': '0.130194', 'train_acc': '95.205997'}]
[Epoch:0335/0600 lr:0.011083]: 100%|██| 521/521 [00:36<00:00, 14.33it/s, log={'train_loss': '0.124806', 'train_acc': '95.491997'}]
[Val_Accuracy epoch:335] val_loss:0.500010, val_acc:86.889997
[Epoch:0336/0600 lr:0.011042]: 100%|██| 521/521 [00:36<00:00, 14.37it/s, log={'train_loss': '0.130379', 'train_acc': '95.359997'}]
[Epoch:0337/0600 lr:0.011000]: 100%|██| 521/521 [00:38<00:00, 13.70it/s, log={'train_loss': '0.126872', 'train_acc': '95.337997'}]
[Epoch:0338/0600 lr:0.010958]: 100%|██| 521/521 [00:35<00:00, 14.67it/s, log={'train_loss': '0.127496', 'train_acc': '95.347997'}]
[Epoch:0339/0600 lr:0.010917]: 100%|██| 521/521 [00:38<00:00, 13.64it/s, log={'train_loss': '0.129409', 'train_acc': '95.179998'}]
[Epoch:0340/0600 lr:0.010875]: 100%|██| 521/521 [00:36<00:00, 14.38it/s, log={'train_loss': '0.125933', 'train_acc': '95.497997'}]
[Val_Accuracy epoch:340] val_loss:0.495482, val_acc:87.309997
[Epoch:0341/0600 lr:0.010833]: 100%|██| 521/521 [00:36<00:00, 14.32it/s, log={'train_loss': '0.119389', 'train_acc': '95.675997'}]
[Epoch:0342/0600 lr:0.010792]: 100%|██| 521/521 [00:36<00:00, 14.35it/s, log={'train_loss': '0.120711', 'train_acc': '95.641998'}]
[Epoch:0343/0600 lr:0.010750]: 100%|██| 521/521 [00:35<00:00, 14.65it/s, log={'train_loss': '0.123990', 'train_acc': '95.465997'}]
[Epoch:0344/0600 lr:0.010708]: 100%|██| 521/521 [00:37<00:00, 14.00it/s, log={'train_loss': '0.121482', 'train_acc': '95.435997'}]
[Epoch:0345/0600 lr:0.010667]: 100%|██| 521/521 [00:37<00:00, 13.78it/s, log={'train_loss': '0.118974', 'train_acc': '95.573997'}]
[Val_Accuracy epoch:345] val_loss:0.446339, val_acc:88.189998
[Epoch:0346/0600 lr:0.010625]: 100%|██| 521/521 [00:36<00:00, 14.11it/s, log={'train_loss': '0.122301', 'train_acc': '95.551997'}]
[Epoch:0347/0600 lr:0.010583]: 100%|██| 521/521 [00:37<00:00, 14.01it/s, log={'train_loss': '0.118974', 'train_acc': '95.645997'}]
[Epoch:0348/0600 lr:0.010542]: 100%|██| 521/521 [00:36<00:00, 14.11it/s, log={'train_loss': '0.118199', 'train_acc': '95.755997'}]
[Epoch:0349/0600 lr:0.010500]: 100%|██| 521/521 [00:36<00:00, 14.16it/s, log={'train_loss': '0.119849', 'train_acc': '95.709997'}]
[Epoch:0350/0600 lr:0.010458]: 100%|██| 521/521 [00:37<00:00, 14.08it/s, log={'train_loss': '0.116962', 'train_acc': '95.739998'}]
[Val_Accuracy epoch:350] val_loss:0.477068, val_acc:87.509997
[Epoch:0351/0600 lr:0.010417]: 100%|██| 521/521 [00:37<00:00, 13.84it/s, log={'train_loss': '0.117079', 'train_acc': '95.727998'}]
[Epoch:0352/0600 lr:0.010375]: 100%|██| 521/521 [00:35<00:00, 14.73it/s, log={'train_loss': '0.114776', 'train_acc': '95.871997'}]
[Epoch:0353/0600 lr:0.010333]: 100%|██| 521/521 [00:35<00:00, 14.57it/s, log={'train_loss': '0.115687', 'train_acc': '95.717997'}]
[Epoch:0354/0600 lr:0.010292]: 100%|██| 521/521 [00:38<00:00, 13.68it/s, log={'train_loss': '0.114431', 'train_acc': '95.895997'}]
[Epoch:0355/0600 lr:0.010250]: 100%|██| 521/521 [00:36<00:00, 14.18it/s, log={'train_loss': '0.111746', 'train_acc': '95.927998'}]
[Val_Accuracy epoch:355] val_loss:0.473931, val_acc:87.419997
[Epoch:0356/0600 lr:0.010208]: 100%|██| 521/521 [00:36<00:00, 14.25it/s, log={'train_loss': '0.112662', 'train_acc': '95.967997'}]
[Epoch:0357/0600 lr:0.010167]: 100%|██| 521/521 [00:36<00:00, 14.12it/s, log={'train_loss': '0.114153', 'train_acc': '95.971997'}]
[Epoch:0358/0600 lr:0.010125]: 100%|██| 521/521 [00:38<00:00, 13.70it/s, log={'train_loss': '0.114881', 'train_acc': '95.899997'}]
[Epoch:0359/0600 lr:0.010083]: 100%|██| 521/521 [00:37<00:00, 13.87it/s, log={'train_loss': '0.109697', 'train_acc': '95.945998'}]
[Epoch:0360/0600 lr:0.010042]: 100%|██| 521/521 [00:36<00:00, 14.22it/s, log={'train_loss': '0.112857', 'train_acc': '95.921998'}]
[Val_Accuracy epoch:360] val_loss:0.505246, val_acc:87.529997
[Epoch:0361/0600 lr:0.010000]: 100%|██| 521/521 [00:37<00:00, 14.02it/s, log={'train_loss': '0.107223', 'train_acc': '96.159998'}]
[Epoch:0362/0600 lr:0.009958]: 100%|██| 521/521 [00:37<00:00, 13.86it/s, log={'train_loss': '0.108269', 'train_acc': '96.151997'}]
[Epoch:0363/0600 lr:0.009917]: 100%|██| 521/521 [00:36<00:00, 14.41it/s, log={'train_loss': '0.109271', 'train_acc': '96.069998'}]
[Epoch:0364/0600 lr:0.009875]: 100%|██| 521/521 [00:35<00:00, 14.63it/s, log={'train_loss': '0.110199', 'train_acc': '96.045998'}]
[Epoch:0365/0600 lr:0.009833]: 100%|██| 521/521 [00:37<00:00, 13.81it/s, log={'train_loss': '0.108769', 'train_acc': '96.037998'}]
[Val_Accuracy epoch:365] val_loss:0.483526, val_acc:87.759997
[Epoch:0366/0600 lr:0.009792]: 100%|██| 521/521 [00:37<00:00, 13.73it/s, log={'train_loss': '0.108944', 'train_acc': '96.001997'}]
[Epoch:0367/0600 lr:0.009750]: 100%|██| 521/521 [00:37<00:00, 14.04it/s, log={'train_loss': '0.104659', 'train_acc': '96.143997'}]
[Epoch:0368/0600 lr:0.009708]: 100%|██| 521/521 [00:36<00:00, 14.13it/s, log={'train_loss': '0.108136', 'train_acc': '96.089997'}]
[Epoch:0369/0600 lr:0.009667]: 100%|██| 521/521 [00:37<00:00, 13.74it/s, log={'train_loss': '0.104486', 'train_acc': '96.297998'}]
[Epoch:0370/0600 lr:0.009625]: 100%|██| 521/521 [00:38<00:00, 13.64it/s, log={'train_loss': '0.102521', 'train_acc': '96.303997'}]
[Val_Accuracy epoch:370] val_loss:0.513294, val_acc:87.089998
[Epoch:0371/0600 lr:0.009583]: 100%|██| 521/521 [00:36<00:00, 14.13it/s, log={'train_loss': '0.104499', 'train_acc': '96.243997'}]
[Epoch:0372/0600 lr:0.009542]: 100%|██| 521/521 [00:36<00:00, 14.16it/s, log={'train_loss': '0.104636', 'train_acc': '96.205997'}]
[Epoch:0373/0600 lr:0.009500]: 100%|██| 521/521 [00:37<00:00, 13.93it/s, log={'train_loss': '0.106060', 'train_acc': '96.059997'}]
[Epoch:0374/0600 lr:0.009458]: 100%|██| 521/521 [00:38<00:00, 13.63it/s, log={'train_loss': '0.102511', 'train_acc': '96.193997'}]
[Epoch:0375/0600 lr:0.009417]: 100%|██| 521/521 [00:37<00:00, 14.02it/s, log={'train_loss': '0.103057', 'train_acc': '96.313997'}]
[Val_Accuracy epoch:375] val_loss:0.511201, val_acc:87.289998
[Epoch:0376/0600 lr:0.009375]: 100%|██| 521/521 [00:35<00:00, 14.65it/s, log={'train_loss': '0.102671', 'train_acc': '96.249997'}]
[Epoch:0377/0600 lr:0.009333]: 100%|██| 521/521 [00:36<00:00, 14.31it/s, log={'train_loss': '0.097458', 'train_acc': '96.425997'}]
[Epoch:0378/0600 lr:0.009292]: 100%|██| 521/521 [00:36<00:00, 14.37it/s, log={'train_loss': '0.099189', 'train_acc': '96.391997'}]
[Epoch:0379/0600 lr:0.009250]: 100%|██| 521/521 [00:37<00:00, 13.92it/s, log={'train_loss': '0.100592', 'train_acc': '96.239997'}]
[Epoch:0380/0600 lr:0.009208]: 100%|██| 521/521 [00:38<00:00, 13.67it/s, log={'train_loss': '0.098195', 'train_acc': '96.391998'}]
[Val_Accuracy epoch:380] val_loss:0.501995, val_acc:87.809997
[Epoch:0381/0600 lr:0.009167]: 100%|██| 521/521 [00:37<00:00, 13.98it/s, log={'train_loss': '0.101554', 'train_acc': '96.401997'}]
[Epoch:0382/0600 lr:0.009125]: 100%|██| 521/521 [00:37<00:00, 14.02it/s, log={'train_loss': '0.097883', 'train_acc': '96.387998'}]
[Epoch:0383/0600 lr:0.009083]: 100%|██| 521/521 [00:35<00:00, 14.81it/s, log={'train_loss': '0.094955', 'train_acc': '96.579998'}]
[Epoch:0384/0600 lr:0.009042]: 100%|██| 521/521 [00:36<00:00, 14.29it/s, log={'train_loss': '0.094913', 'train_acc': '96.495997'}]
[Epoch:0385/0600 lr:0.009000]: 100%|██| 521/521 [00:35<00:00, 14.49it/s, log={'train_loss': '0.096022', 'train_acc': '96.565997'}]
[Val_Accuracy epoch:385] val_loss:0.499874, val_acc:88.109997
[Epoch:0386/0600 lr:0.008958]: 100%|██| 521/521 [00:37<00:00, 13.85it/s, log={'train_loss': '0.094681', 'train_acc': '96.595997'}]
[Epoch:0387/0600 lr:0.008917]: 100%|██| 521/521 [00:36<00:00, 14.15it/s, log={'train_loss': '0.092978', 'train_acc': '96.545997'}]
[Epoch:0388/0600 lr:0.008875]: 100%|██| 521/521 [00:36<00:00, 14.40it/s, log={'train_loss': '0.093560', 'train_acc': '96.679998'}]
[Epoch:0389/0600 lr:0.008833]: 100%|██| 521/521 [00:37<00:00, 13.79it/s, log={'train_loss': '0.093602', 'train_acc': '96.573997'}]
[Epoch:0390/0600 lr:0.008792]: 100%|██| 521/521 [00:37<00:00, 13.85it/s, log={'train_loss': '0.093759', 'train_acc': '96.571998'}]
[Val_Accuracy epoch:390] val_loss:0.493006, val_acc:87.639997
[Epoch:0391/0600 lr:0.008750]: 100%|██| 521/521 [00:37<00:00, 13.82it/s, log={'train_loss': '0.091193', 'train_acc': '96.669997'}]
[Epoch:0392/0600 lr:0.008708]: 100%|██| 521/521 [00:36<00:00, 14.26it/s, log={'train_loss': '0.096006', 'train_acc': '96.567997'}]
[Epoch:0393/0600 lr:0.008667]: 100%|██| 521/521 [00:37<00:00, 13.94it/s, log={'train_loss': '0.094219', 'train_acc': '96.513997'}]
[Epoch:0394/0600 lr:0.008625]: 100%|██| 521/521 [00:36<00:00, 14.41it/s, log={'train_loss': '0.090133', 'train_acc': '96.785998'}]
[Epoch:0395/0600 lr:0.008583]: 100%|██| 521/521 [00:36<00:00, 14.12it/s, log={'train_loss': '0.091013', 'train_acc': '96.711997'}]
[Val_Accuracy epoch:395] val_loss:0.507607, val_acc:87.459997
[Epoch:0396/0600 lr:0.008542]: 100%|██| 521/521 [00:37<00:00, 14.04it/s, log={'train_loss': '0.092153', 'train_acc': '96.675997'}]
[Epoch:0397/0600 lr:0.008500]: 100%|██| 521/521 [00:35<00:00, 14.79it/s, log={'train_loss': '0.090664', 'train_acc': '96.677998'}]
[Epoch:0398/0600 lr:0.008458]: 100%|██| 521/521 [00:38<00:00, 13.61it/s, log={'train_loss': '0.090863', 'train_acc': '96.641997'}]
[Epoch:0399/0600 lr:0.008417]: 100%|██| 521/521 [00:38<00:00, 13.67it/s, log={'train_loss': '0.088806', 'train_acc': '96.761997'}]
[Epoch:0400/0600 lr:0.008375]: 100%|██| 521/521 [00:38<00:00, 13.68it/s, log={'train_loss': '0.090343', 'train_acc': '96.711997'}]
[Val_Accuracy epoch:400] val_loss:0.480839, val_acc:88.149997
[Epoch:0401/0600 lr:0.008333]: 100%|██| 521/521 [00:35<00:00, 14.48it/s, log={'train_loss': '0.086831', 'train_acc': '96.853997'}]
[Epoch:0402/0600 lr:0.008292]: 100%|██| 521/521 [00:37<00:00, 14.08it/s, log={'train_loss': '0.085684', 'train_acc': '96.869997'}]
[Epoch:0403/0600 lr:0.008250]: 100%|██| 521/521 [00:37<00:00, 13.89it/s, log={'train_loss': '0.086521', 'train_acc': '96.953998'}]
[Epoch:0404/0600 lr:0.008208]: 100%|██| 521/521 [00:37<00:00, 14.02it/s, log={'train_loss': '0.087130', 'train_acc': '96.861997'}]
[Epoch:0405/0600 lr:0.008167]: 100%|██| 521/521 [00:37<00:00, 13.91it/s, log={'train_loss': '0.084605', 'train_acc': '96.951997'}]
[Val_Accuracy epoch:405] val_loss:0.499544, val_acc:87.579997
[Epoch:0406/0600 lr:0.008125]: 100%|██| 521/521 [00:36<00:00, 14.22it/s, log={'train_loss': '0.083062', 'train_acc': '97.095998'}]
[Epoch:0407/0600 lr:0.008083]: 100%|██| 521/521 [00:37<00:00, 14.07it/s, log={'train_loss': '0.083685', 'train_acc': '97.015997'}]
[Epoch:0408/0600 lr:0.008042]: 100%|██| 521/521 [00:37<00:00, 13.99it/s, log={'train_loss': '0.082943', 'train_acc': '97.003997'}]
[Epoch:0409/0600 lr:0.008000]: 100%|██| 521/521 [00:35<00:00, 14.50it/s, log={'train_loss': '0.082873', 'train_acc': '96.947997'}]
[Epoch:0410/0600 lr:0.007958]: 100%|██| 521/521 [00:36<00:00, 14.35it/s, log={'train_loss': '0.082199', 'train_acc': '97.141997'}]
[Val_Accuracy epoch:410] val_loss:0.513098, val_acc:87.589998
[Epoch:0411/0600 lr:0.007917]: 100%|██| 521/521 [00:37<00:00, 14.07it/s, log={'train_loss': '0.082017', 'train_acc': '97.049998'}]
[Epoch:0412/0600 lr:0.007875]: 100%|██| 521/521 [00:35<00:00, 14.75it/s, log={'train_loss': '0.082138', 'train_acc': '97.091997'}]
[Epoch:0413/0600 lr:0.007833]: 100%|██| 521/521 [00:37<00:00, 13.76it/s, log={'train_loss': '0.083479', 'train_acc': '96.963998'}]
[Epoch:0414/0600 lr:0.007792]: 100%|██| 521/521 [00:35<00:00, 14.60it/s, log={'train_loss': '0.082109', 'train_acc': '96.979997'}]
[Epoch:0415/0600 lr:0.007750]: 100%|██| 521/521 [00:36<00:00, 14.16it/s, log={'train_loss': '0.080778', 'train_acc': '97.105997'}]
[Val_Accuracy epoch:415] val_loss:0.517606, val_acc:87.579998
[Epoch:0416/0600 lr:0.007708]: 100%|██| 521/521 [00:37<00:00, 13.85it/s, log={'train_loss': '0.081657', 'train_acc': '96.995998'}]
[Epoch:0417/0600 lr:0.007667]: 100%|██| 521/521 [00:36<00:00, 14.44it/s, log={'train_loss': '0.079232', 'train_acc': '97.173997'}]
[Epoch:0418/0600 lr:0.007625]: 100%|██| 521/521 [00:35<00:00, 14.69it/s, log={'train_loss': '0.080036', 'train_acc': '97.205998'}]
[Epoch:0419/0600 lr:0.007583]: 100%|██| 521/521 [00:38<00:00, 13.62it/s, log={'train_loss': '0.078974', 'train_acc': '97.189997'}]
[Epoch:0420/0600 lr:0.007542]: 100%|██| 521/521 [00:36<00:00, 14.21it/s, log={'train_loss': '0.078758', 'train_acc': '97.107997'}]
[Val_Accuracy epoch:420] val_loss:0.486435, val_acc:87.939997
[Epoch:0421/0600 lr:0.007500]: 100%|██| 521/521 [00:37<00:00, 13.83it/s, log={'train_loss': '0.079710', 'train_acc': '97.151997'}]
[Epoch:0422/0600 lr:0.007458]: 100%|██| 521/521 [00:36<00:00, 14.35it/s, log={'train_loss': '0.076055', 'train_acc': '97.181997'}]
[Epoch:0423/0600 lr:0.007417]: 100%|██| 521/521 [00:38<00:00, 13.50it/s, log={'train_loss': '0.078875', 'train_acc': '97.205998'}]
[Epoch:0424/0600 lr:0.007375]: 100%|██| 521/521 [00:38<00:00, 13.66it/s, log={'train_loss': '0.074947', 'train_acc': '97.317997'}]
[Epoch:0425/0600 lr:0.007333]: 100%|██| 521/521 [00:37<00:00, 14.00it/s, log={'train_loss': '0.077113', 'train_acc': '97.251997'}]
[Val_Accuracy epoch:425] val_loss:0.540241, val_acc:87.569997
[Epoch:0426/0600 lr:0.007292]: 100%|██| 521/521 [00:37<00:00, 13.79it/s, log={'train_loss': '0.073398', 'train_acc': '97.323997'}]
[Epoch:0427/0600 lr:0.007250]: 100%|██| 521/521 [00:38<00:00, 13.62it/s, log={'train_loss': '0.077776', 'train_acc': '97.207997'}]
[Epoch:0428/0600 lr:0.007208]: 100%|██| 521/521 [00:38<00:00, 13.54it/s, log={'train_loss': '0.075709', 'train_acc': '97.331997'}]
[Epoch:0429/0600 lr:0.007167]: 100%|██| 521/521 [00:37<00:00, 13.87it/s, log={'train_loss': '0.075480', 'train_acc': '97.297997'}]
[Epoch:0430/0600 lr:0.007125]: 100%|██| 521/521 [00:37<00:00, 14.04it/s, log={'train_loss': '0.073894', 'train_acc': '97.307997'}]
[Val_Accuracy epoch:430] val_loss:0.487931, val_acc:88.269998
[Epoch:0431/0600 lr:0.007083]: 100%|██| 521/521 [00:35<00:00, 14.59it/s, log={'train_loss': '0.074600', 'train_acc': '97.369997'}]
[Epoch:0432/0600 lr:0.007042]: 100%|██| 521/521 [00:36<00:00, 14.23it/s, log={'train_loss': '0.073956', 'train_acc': '97.343998'}]
[Epoch:0433/0600 lr:0.007000]: 100%|██| 521/521 [00:36<00:00, 14.23it/s, log={'train_loss': '0.071695', 'train_acc': '97.529997'}]
[Epoch:0434/0600 lr:0.006958]: 100%|██| 521/521 [00:38<00:00, 13.69it/s, log={'train_loss': '0.071341', 'train_acc': '97.385997'}]
[Epoch:0435/0600 lr:0.006917]: 100%|██| 521/521 [00:36<00:00, 14.28it/s, log={'train_loss': '0.071442', 'train_acc': '97.481997'}]
[Val_Accuracy epoch:435] val_loss:0.488451, val_acc:88.159997
[Epoch:0436/0600 lr:0.006875]: 100%|██| 521/521 [00:37<00:00, 13.86it/s, log={'train_loss': '0.074043', 'train_acc': '97.433997'}]
[Epoch:0437/0600 lr:0.006833]: 100%|██| 521/521 [00:37<00:00, 13.97it/s, log={'train_loss': '0.072275', 'train_acc': '97.463997'}]
[Epoch:0438/0600 lr:0.006792]: 100%|██| 521/521 [00:35<00:00, 14.51it/s, log={'train_loss': '0.073662', 'train_acc': '97.397997'}]
[Epoch:0439/0600 lr:0.006750]: 100%|██| 521/521 [00:36<00:00, 14.25it/s, log={'train_loss': '0.071446', 'train_acc': '97.475997'}]
[Epoch:0440/0600 lr:0.006708]: 100%|██| 521/521 [00:36<00:00, 14.39it/s, log={'train_loss': '0.069721', 'train_acc': '97.523997'}]
[Val_Accuracy epoch:440] val_loss:0.516084, val_acc:88.309998
[Epoch:0441/0600 lr:0.006667]: 100%|██| 521/521 [00:35<00:00, 14.71it/s, log={'train_loss': '0.071485', 'train_acc': '97.461997'}]
[Epoch:0442/0600 lr:0.006625]: 100%|██| 521/521 [00:35<00:00, 14.54it/s, log={'train_loss': '0.068547', 'train_acc': '97.579998'}]
[Epoch:0443/0600 lr:0.006583]: 100%|██| 521/521 [00:36<00:00, 14.22it/s, log={'train_loss': '0.069135', 'train_acc': '97.573997'}]
[Epoch:0444/0600 lr:0.006542]: 100%|██| 521/521 [00:37<00:00, 13.85it/s, log={'train_loss': '0.070160', 'train_acc': '97.517997'}]
[Epoch:0445/0600 lr:0.006500]: 100%|██| 521/521 [00:37<00:00, 13.82it/s, log={'train_loss': '0.070629', 'train_acc': '97.515997'}]
[Val_Accuracy epoch:445] val_loss:0.508120, val_acc:87.749997
[Epoch:0446/0600 lr:0.006458]: 100%|██| 521/521 [00:37<00:00, 13.78it/s, log={'train_loss': '0.069164', 'train_acc': '97.521998'}]
[Epoch:0447/0600 lr:0.006417]: 100%|██| 521/521 [00:37<00:00, 13.84it/s, log={'train_loss': '0.068197', 'train_acc': '97.599997'}]
[Epoch:0448/0600 lr:0.006375]: 100%|██| 521/521 [00:37<00:00, 13.80it/s, log={'train_loss': '0.067370', 'train_acc': '97.597997'}]
[Epoch:0449/0600 lr:0.006333]: 100%|██| 521/521 [00:38<00:00, 13.65it/s, log={'train_loss': '0.066557', 'train_acc': '97.661997'}]
[Epoch:0450/0600 lr:0.006292]: 100%|██| 521/521 [00:36<00:00, 14.36it/s, log={'train_loss': '0.065188', 'train_acc': '97.645997'}]
[Val_Accuracy epoch:450] val_loss:0.523550, val_acc:87.689997
[Epoch:0451/0600 lr:0.006250]: 100%|██| 521/521 [00:37<00:00, 13.81it/s, log={'train_loss': '0.065429', 'train_acc': '97.715998'}]
[Epoch:0452/0600 lr:0.006208]: 100%|██| 521/521 [00:36<00:00, 14.12it/s, log={'train_loss': '0.066803', 'train_acc': '97.613997'}]
[Epoch:0453/0600 lr:0.006167]: 100%|██| 521/521 [00:36<00:00, 14.10it/s, log={'train_loss': '0.066320', 'train_acc': '97.557997'}]
[Epoch:0454/0600 lr:0.006125]: 100%|██| 521/521 [00:38<00:00, 13.55it/s, log={'train_loss': '0.066092', 'train_acc': '97.711997'}]
[Epoch:0455/0600 lr:0.006083]: 100%|██| 521/521 [00:37<00:00, 13.77it/s, log={'train_loss': '0.065831', 'train_acc': '97.679997'}]
[Val_Accuracy epoch:455] val_loss:0.469338, val_acc:88.639998
[Epoch:0456/0600 lr:0.006042]: 100%|██| 521/521 [00:36<00:00, 14.41it/s, log={'train_loss': '0.065168', 'train_acc': '97.755997'}]
[Epoch:0457/0600 lr:0.006000]: 100%|██| 521/521 [00:36<00:00, 14.19it/s, log={'train_loss': '0.065061', 'train_acc': '97.689997'}]
[Epoch:0458/0600 lr:0.005958]: 100%|██| 521/521 [00:38<00:00, 13.70it/s, log={'train_loss': '0.060804', 'train_acc': '97.839997'}]
[Epoch:0459/0600 lr:0.005917]: 100%|██| 521/521 [00:35<00:00, 14.52it/s, log={'train_loss': '0.061361', 'train_acc': '97.795997'}]
[Epoch:0460/0600 lr:0.005875]: 100%|██| 521/521 [00:36<00:00, 14.37it/s, log={'train_loss': '0.062546', 'train_acc': '97.813997'}]
[Val_Accuracy epoch:460] val_loss:0.486585, val_acc:88.649998
[Epoch:0461/0600 lr:0.005833]: 100%|██| 521/521 [00:36<00:00, 14.44it/s, log={'train_loss': '0.062067', 'train_acc': '97.801997'}]
[Epoch:0462/0600 lr:0.005792]: 100%|██| 521/521 [00:36<00:00, 14.20it/s, log={'train_loss': '0.060799', 'train_acc': '97.799997'}]
[Epoch:0463/0600 lr:0.005750]: 100%|██| 521/521 [00:37<00:00, 13.72it/s, log={'train_loss': '0.060730', 'train_acc': '97.871997'}]
[Epoch:0464/0600 lr:0.005708]: 100%|██| 521/521 [00:38<00:00, 13.60it/s, log={'train_loss': '0.060760', 'train_acc': '97.851997'}]
[Epoch:0465/0600 lr:0.005667]: 100%|██| 521/521 [00:36<00:00, 14.30it/s, log={'train_loss': '0.060407', 'train_acc': '97.807997'}]
[Val_Accuracy epoch:465] val_loss:0.485988, val_acc:88.149997
[Epoch:0466/0600 lr:0.005625]: 100%|██| 521/521 [00:37<00:00, 13.90it/s, log={'train_loss': '0.060049', 'train_acc': '97.881997'}]
[Epoch:0467/0600 lr:0.005583]: 100%|██| 521/521 [00:35<00:00, 14.73it/s, log={'train_loss': '0.062351', 'train_acc': '97.801997'}]
[Epoch:0468/0600 lr:0.005542]: 100%|██| 521/521 [00:37<00:00, 13.88it/s, log={'train_loss': '0.062109', 'train_acc': '97.785997'}]
[Epoch:0469/0600 lr:0.005500]: 100%|██| 521/521 [00:37<00:00, 13.88it/s, log={'train_loss': '0.061234', 'train_acc': '97.803997'}]
[Epoch:0470/0600 lr:0.005458]: 100%|██| 521/521 [00:38<00:00, 13.71it/s, log={'train_loss': '0.060055', 'train_acc': '97.935997'}]
[Val_Accuracy epoch:470] val_loss:0.515087, val_acc:88.519997
[Epoch:0471/0600 lr:0.005417]: 100%|██| 521/521 [00:37<00:00, 13.86it/s, log={'train_loss': '0.059549', 'train_acc': '97.865997'}]
[Epoch:0472/0600 lr:0.005375]: 100%|██| 521/521 [00:37<00:00, 13.72it/s, log={'train_loss': '0.055809', 'train_acc': '98.001997'}]
[Epoch:0473/0600 lr:0.005333]: 100%|██| 521/521 [00:36<00:00, 14.22it/s, log={'train_loss': '0.058131', 'train_acc': '97.931997'}]
[Epoch:0474/0600 lr:0.005292]: 100%|██| 521/521 [00:38<00:00, 13.65it/s, log={'train_loss': '0.058060', 'train_acc': '97.949997'}]
[Epoch:0475/0600 lr:0.005250]: 100%|██| 521/521 [00:35<00:00, 14.53it/s, log={'train_loss': '0.055435', 'train_acc': '98.063997'}]
[Val_Accuracy epoch:475] val_loss:0.504091, val_acc:88.429997
[Epoch:0476/0600 lr:0.005208]: 100%|██| 521/521 [00:37<00:00, 13.74it/s, log={'train_loss': '0.056571', 'train_acc': '97.997997'}]
[Epoch:0477/0600 lr:0.005167]: 100%|██| 521/521 [00:38<00:00, 13.61it/s, log={'train_loss': '0.055786', 'train_acc': '98.027997'}]
[Epoch:0478/0600 lr:0.005125]: 100%|██| 521/521 [00:36<00:00, 14.37it/s, log={'train_loss': '0.057603', 'train_acc': '97.943997'}]
[Epoch:0479/0600 lr:0.005083]: 100%|██| 521/521 [00:37<00:00, 14.00it/s, log={'train_loss': '0.056686', 'train_acc': '97.949997'}]
[Epoch:0480/0600 lr:0.005042]: 100%|██| 521/521 [00:37<00:00, 13.89it/s, log={'train_loss': '0.056578', 'train_acc': '98.011997'}]
[Val_Accuracy epoch:480] val_loss:0.512041, val_acc:87.759997
[Epoch:0481/0600 lr:0.005000]: 100%|██| 521/521 [00:36<00:00, 14.30it/s, log={'train_loss': '0.054932', 'train_acc': '98.049997'}]
[Epoch:0482/0600 lr:0.004958]: 100%|██| 521/521 [00:37<00:00, 13.96it/s, log={'train_loss': '0.054355', 'train_acc': '98.137997'}]
[Epoch:0483/0600 lr:0.004917]: 100%|██| 521/521 [00:37<00:00, 13.76it/s, log={'train_loss': '0.055384', 'train_acc': '98.037997'}]
[Epoch:0484/0600 lr:0.004875]: 100%|██| 521/521 [00:36<00:00, 14.15it/s, log={'train_loss': '0.055776', 'train_acc': '98.053997'}]
[Epoch:0485/0600 lr:0.004833]: 100%|██| 521/521 [00:37<00:00, 14.05it/s, log={'train_loss': '0.055286', 'train_acc': '98.085997'}]
[Val_Accuracy epoch:485] val_loss:0.523529, val_acc:87.929997
[Epoch:0486/0600 lr:0.004792]: 100%|██| 521/521 [00:37<00:00, 13.96it/s, log={'train_loss': '0.053607', 'train_acc': '98.157998'}]
[Epoch:0487/0600 lr:0.004750]: 100%|██| 521/521 [00:37<00:00, 13.93it/s, log={'train_loss': '0.054708', 'train_acc': '98.077997'}]
[Epoch:0488/0600 lr:0.004708]: 100%|██| 521/521 [00:37<00:00, 13.72it/s, log={'train_loss': '0.055171', 'train_acc': '98.091997'}]
[Epoch:0489/0600 lr:0.004667]: 100%|██| 521/521 [00:38<00:00, 13.68it/s, log={'train_loss': '0.054724', 'train_acc': '98.103997'}]
[Epoch:0490/0600 lr:0.004625]: 100%|██| 521/521 [00:36<00:00, 14.35it/s, log={'train_loss': '0.053954', 'train_acc': '98.111997'}]
[Val_Accuracy epoch:490] val_loss:0.509158, val_acc:88.589998
[Epoch:0491/0600 lr:0.004583]: 100%|██| 521/521 [00:37<00:00, 13.89it/s, log={'train_loss': '0.053595', 'train_acc': '98.131997'}]
[Epoch:0492/0600 lr:0.004542]: 100%|██| 521/521 [00:38<00:00, 13.58it/s, log={'train_loss': '0.051192', 'train_acc': '98.261997'}]
[Epoch:0493/0600 lr:0.004500]: 100%|██| 521/521 [00:36<00:00, 14.19it/s, log={'train_loss': '0.054531', 'train_acc': '98.105997'}]
[Epoch:0494/0600 lr:0.004458]: 100%|██| 521/521 [00:36<00:00, 14.20it/s, log={'train_loss': '0.051739', 'train_acc': '98.171997'}]
[Epoch:0495/0600 lr:0.004417]: 100%|██| 521/521 [00:35<00:00, 14.72it/s, log={'train_loss': '0.051128', 'train_acc': '98.243997'}]
[Val_Accuracy epoch:495] val_loss:0.491907, val_acc:88.469997
[Epoch:0496/0600 lr:0.004375]: 100%|██| 521/521 [00:37<00:00, 13.89it/s, log={'train_loss': '0.051720', 'train_acc': '98.159997'}]
[Epoch:0497/0600 lr:0.004333]: 100%|██| 521/521 [00:35<00:00, 14.87it/s, log={'train_loss': '0.048926', 'train_acc': '98.279997'}]
[Epoch:0498/0600 lr:0.004292]: 100%|██| 521/521 [00:37<00:00, 13.81it/s, log={'train_loss': '0.049607', 'train_acc': '98.337997'}]
[Epoch:0499/0600 lr:0.004250]: 100%|██| 521/521 [00:36<00:00, 14.45it/s, log={'train_loss': '0.052224', 'train_acc': '98.101997'}]
[Epoch:0500/0600 lr:0.004208]: 100%|██| 521/521 [00:36<00:00, 14.26it/s, log={'train_loss': '0.050368', 'train_acc': '98.249997'}]
[Val_Accuracy epoch:500] val_loss:0.526085, val_acc:88.239998
[Epoch:0501/0600 lr:0.004167]: 100%|██| 521/521 [00:37<00:00, 13.88it/s, log={'train_loss': '0.050544', 'train_acc': '98.269997'}]
[Epoch:0502/0600 lr:0.004125]: 100%|██| 521/521 [00:37<00:00, 13.84it/s, log={'train_loss': '0.049746', 'train_acc': '98.267997'}]
[Epoch:0503/0600 lr:0.004083]: 100%|██| 521/521 [00:35<00:00, 14.57it/s, log={'train_loss': '0.050893', 'train_acc': '98.201997'}]
[Epoch:0504/0600 lr:0.004042]: 100%|██| 521/521 [00:35<00:00, 14.66it/s, log={'train_loss': '0.050880', 'train_acc': '98.231997'}]
[Epoch:0505/0600 lr:0.004000]: 100%|██| 521/521 [00:37<00:00, 13.87it/s, log={'train_loss': '0.052695', 'train_acc': '98.173997'}]
[Val_Accuracy epoch:505] val_loss:0.549987, val_acc:88.019997
[Epoch:0506/0600 lr:0.003958]: 100%|██| 521/521 [00:36<00:00, 14.13it/s, log={'train_loss': '0.048613', 'train_acc': '98.299997'}]
[Epoch:0507/0600 lr:0.003917]: 100%|██| 521/521 [00:37<00:00, 14.00it/s, log={'train_loss': '0.049062', 'train_acc': '98.251997'}]
[Epoch:0508/0600 lr:0.003875]: 100%|██| 521/521 [00:36<00:00, 14.41it/s, log={'train_loss': '0.047915', 'train_acc': '98.329997'}]
[Epoch:0509/0600 lr:0.003833]: 100%|██| 521/521 [00:36<00:00, 14.11it/s, log={'train_loss': '0.050426', 'train_acc': '98.239997'}]
[Epoch:0510/0600 lr:0.003792]: 100%|██| 521/521 [00:37<00:00, 13.90it/s, log={'train_loss': '0.050871', 'train_acc': '98.181998'}]
[Val_Accuracy epoch:510] val_loss:0.499242, val_acc:88.619997
[Epoch:0511/0600 lr:0.003750]: 100%|██| 521/521 [00:38<00:00, 13.70it/s, log={'train_loss': '0.049998', 'train_acc': '98.219997'}]
[Epoch:0512/0600 lr:0.003708]: 100%|██| 521/521 [00:36<00:00, 14.42it/s, log={'train_loss': '0.048924', 'train_acc': '98.297997'}]
[Epoch:0513/0600 lr:0.003667]: 100%|██| 521/521 [00:36<00:00, 14.35it/s, log={'train_loss': '0.046412', 'train_acc': '98.413997'}]
[Epoch:0514/0600 lr:0.003625]: 100%|██| 521/521 [00:36<00:00, 14.43it/s, log={'train_loss': '0.046885', 'train_acc': '98.379997'}]
[Epoch:0515/0600 lr:0.003583]: 100%|██| 521/521 [00:38<00:00, 13.69it/s, log={'train_loss': '0.046832', 'train_acc': '98.415997'}]
[Val_Accuracy epoch:515] val_loss:0.519622, val_acc:88.529997
[Epoch:0516/0600 lr:0.003542]: 100%|██| 521/521 [00:35<00:00, 14.61it/s, log={'train_loss': '0.047735', 'train_acc': '98.337997'}]
[Epoch:0517/0600 lr:0.003500]: 100%|██| 521/521 [00:36<00:00, 14.33it/s, log={'train_loss': '0.046814', 'train_acc': '98.371997'}]
[Epoch:0518/0600 lr:0.003458]: 100%|██| 521/521 [00:37<00:00, 14.07it/s, log={'train_loss': '0.047045', 'train_acc': '98.395997'}]
[Epoch:0519/0600 lr:0.003417]: 100%|██| 521/521 [00:37<00:00, 13.79it/s, log={'train_loss': '0.046100', 'train_acc': '98.433997'}]
[Epoch:0520/0600 lr:0.003375]: 100%|██| 521/521 [00:36<00:00, 14.36it/s, log={'train_loss': '0.045806', 'train_acc': '98.439997'}]
[Val_Accuracy epoch:520] val_loss:0.529534, val_acc:88.349997
[Epoch:0521/0600 lr:0.003333]: 100%|██| 521/521 [00:36<00:00, 14.14it/s, log={'train_loss': '0.044559', 'train_acc': '98.435997'}]
[Epoch:0522/0600 lr:0.003292]: 100%|██| 521/521 [00:37<00:00, 13.76it/s, log={'train_loss': '0.045795', 'train_acc': '98.469997'}]
[Epoch:0523/0600 lr:0.003250]: 100%|██| 521/521 [00:37<00:00, 14.03it/s, log={'train_loss': '0.045447', 'train_acc': '98.349998'}]
[Epoch:0524/0600 lr:0.003208]: 100%|██| 521/521 [00:37<00:00, 14.01it/s, log={'train_loss': '0.044830', 'train_acc': '98.443997'}]
[Epoch:0525/0600 lr:0.003167]: 100%|██| 521/521 [00:37<00:00, 13.97it/s, log={'train_loss': '0.043923', 'train_acc': '98.515997'}]
[Val_Accuracy epoch:525] val_loss:0.529406, val_acc:88.319997
[Epoch:0526/0600 lr:0.003125]: 100%|██| 521/521 [00:36<00:00, 14.20it/s, log={'train_loss': '0.045494', 'train_acc': '98.461998'}]
[Epoch:0527/0600 lr:0.003083]: 100%|██| 521/521 [00:37<00:00, 13.77it/s, log={'train_loss': '0.045958', 'train_acc': '98.377997'}]
[Epoch:0528/0600 lr:0.003042]: 100%|██| 521/521 [00:38<00:00, 13.58it/s, log={'train_loss': '0.046577', 'train_acc': '98.465998'}]
[Epoch:0529/0600 lr:0.003000]: 100%|██| 521/521 [00:37<00:00, 13.73it/s, log={'train_loss': '0.045899', 'train_acc': '98.439997'}]
[Epoch:0530/0600 lr:0.002958]: 100%|██| 521/521 [00:38<00:00, 13.57it/s, log={'train_loss': '0.042705', 'train_acc': '98.483997'}]
[Val_Accuracy epoch:530] val_loss:0.484510, val_acc:88.779997
[Epoch:0531/0600 lr:0.002917]: 100%|██| 521/521 [00:36<00:00, 14.46it/s, log={'train_loss': '0.045844', 'train_acc': '98.415998'}]
[Epoch:0532/0600 lr:0.002875]: 100%|██| 521/521 [00:36<00:00, 14.29it/s, log={'train_loss': '0.042409', 'train_acc': '98.537998'}]
[Epoch:0533/0600 lr:0.002833]: 100%|██| 521/521 [00:35<00:00, 14.56it/s, log={'train_loss': '0.042726', 'train_acc': '98.517997'}]
[Epoch:0534/0600 lr:0.002792]: 100%|██| 521/521 [00:38<00:00, 13.52it/s, log={'train_loss': '0.043032', 'train_acc': '98.563997'}]
[Epoch:0535/0600 lr:0.002750]: 100%|██| 521/521 [00:36<00:00, 14.21it/s, log={'train_loss': '0.044516', 'train_acc': '98.493998'}]
[Val_Accuracy epoch:535] val_loss:0.515194, val_acc:88.769997
[Epoch:0536/0600 lr:0.002708]: 100%|██| 521/521 [00:37<00:00, 13.98it/s, log={'train_loss': '0.042794', 'train_acc': '98.501997'}]
[Epoch:0537/0600 lr:0.002667]: 100%|██| 521/521 [00:36<00:00, 14.24it/s, log={'train_loss': '0.042646', 'train_acc': '98.531997'}]
[Epoch:0538/0600 lr:0.002625]: 100%|██| 521/521 [00:37<00:00, 13.90it/s, log={'train_loss': '0.043896', 'train_acc': '98.517997'}]
[Epoch:0539/0600 lr:0.002583]: 100%|██| 521/521 [00:37<00:00, 14.07it/s, log={'train_loss': '0.043957', 'train_acc': '98.519997'}]
[Epoch:0540/0600 lr:0.002542]: 100%|██| 521/521 [00:36<00:00, 14.40it/s, log={'train_loss': '0.041971', 'train_acc': '98.559998'}]
[Val_Accuracy epoch:540] val_loss:0.506121, val_acc:88.229997
[Epoch:0541/0600 lr:0.002500]: 100%|██| 521/521 [00:35<00:00, 14.76it/s, log={'train_loss': '0.041574', 'train_acc': '98.565997'}]
[Epoch:0542/0600 lr:0.002458]: 100%|██| 521/521 [00:38<00:00, 13.58it/s, log={'train_loss': '0.042744', 'train_acc': '98.557998'}]
[Epoch:0543/0600 lr:0.002417]: 100%|██| 521/521 [00:35<00:00, 14.56it/s, log={'train_loss': '0.042136', 'train_acc': '98.561997'}]
[Epoch:0544/0600 lr:0.002375]: 100%|██| 521/521 [00:37<00:00, 13.87it/s, log={'train_loss': '0.042603', 'train_acc': '98.517997'}]
[Epoch:0545/0600 lr:0.002333]: 100%|██| 521/521 [00:38<00:00, 13.46it/s, log={'train_loss': '0.041975', 'train_acc': '98.575998'}]
[Val_Accuracy epoch:545] val_loss:0.515303, val_acc:88.609998
[Epoch:0546/0600 lr:0.002292]: 100%|██| 521/521 [00:37<00:00, 13.89it/s, log={'train_loss': '0.041145', 'train_acc': '98.537997'}]
[Epoch:0547/0600 lr:0.002250]: 100%|██| 521/521 [00:36<00:00, 14.34it/s, log={'train_loss': '0.041486', 'train_acc': '98.605998'}]
[Epoch:0548/0600 lr:0.002208]: 100%|██| 521/521 [00:37<00:00, 13.87it/s, log={'train_loss': '0.041629', 'train_acc': '98.585997'}]
[Epoch:0549/0600 lr:0.002167]: 100%|██| 521/521 [00:36<00:00, 14.33it/s, log={'train_loss': '0.042479', 'train_acc': '98.547997'}]
[Epoch:0550/0600 lr:0.002125]: 100%|██| 521/521 [00:37<00:00, 14.05it/s, log={'train_loss': '0.041164', 'train_acc': '98.601997'}]
[Val_Accuracy epoch:550] val_loss:0.490540, val_acc:88.929997
[Epoch:0551/0600 lr:0.002083]: 100%|██| 521/521 [00:37<00:00, 13.80it/s, log={'train_loss': '0.040596', 'train_acc': '98.653997'}]
[Epoch:0552/0600 lr:0.002042]: 100%|██| 521/521 [00:36<00:00, 14.47it/s, log={'train_loss': '0.041852', 'train_acc': '98.551997'}]
[Epoch:0553/0600 lr:0.002000]: 100%|██| 521/521 [00:37<00:00, 13.77it/s, log={'train_loss': '0.041032', 'train_acc': '98.665997'}]
[Epoch:0554/0600 lr:0.001958]: 100%|██| 521/521 [00:36<00:00, 14.24it/s, log={'train_loss': '0.038103', 'train_acc': '98.719998'}]
[Epoch:0555/0600 lr:0.001917]: 100%|██| 521/521 [00:36<00:00, 14.22it/s, log={'train_loss': '0.040500', 'train_acc': '98.625997'}]
[Val_Accuracy epoch:555] val_loss:0.496802, val_acc:88.709997
[Epoch:0556/0600 lr:0.001875]: 100%|██| 521/521 [00:36<00:00, 14.34it/s, log={'train_loss': '0.039094', 'train_acc': '98.693997'}]
[Epoch:0557/0600 lr:0.001833]: 100%|██| 521/521 [00:37<00:00, 13.84it/s, log={'train_loss': '0.038801', 'train_acc': '98.651997'}]
[Epoch:0558/0600 lr:0.001792]: 100%|██| 521/521 [00:37<00:00, 13.81it/s, log={'train_loss': '0.037274', 'train_acc': '98.765997'}]
[Epoch:0559/0600 lr:0.001750]: 100%|██| 521/521 [00:35<00:00, 14.53it/s, log={'train_loss': '0.039695', 'train_acc': '98.633998'}]
[Epoch:0560/0600 lr:0.001708]: 100%|██| 521/521 [00:35<00:00, 14.60it/s, log={'train_loss': '0.038563', 'train_acc': '98.721997'}]
[Val_Accuracy epoch:560] val_loss:0.537218, val_acc:88.329998
[Epoch:0561/0600 lr:0.001667]: 100%|██| 521/521 [00:38<00:00, 13.69it/s, log={'train_loss': '0.039452', 'train_acc': '98.647997'}]
[Epoch:0562/0600 lr:0.001625]: 100%|██| 521/521 [00:36<00:00, 14.43it/s, log={'train_loss': '0.037102', 'train_acc': '98.777997'}]
[Epoch:0563/0600 lr:0.001583]: 100%|██| 521/521 [00:37<00:00, 13.90it/s, log={'train_loss': '0.039515', 'train_acc': '98.641998'}]
[Epoch:0564/0600 lr:0.001542]: 100%|██| 521/521 [00:37<00:00, 13.78it/s, log={'train_loss': '0.040371', 'train_acc': '98.611998'}]
[Epoch:0565/0600 lr:0.001500]: 100%|██| 521/521 [00:37<00:00, 13.94it/s, log={'train_loss': '0.039195', 'train_acc': '98.677998'}]
[Val_Accuracy epoch:565] val_loss:0.522554, val_acc:88.309997
[Epoch:0566/0600 lr:0.001458]: 100%|██| 521/521 [00:38<00:00, 13.64it/s, log={'train_loss': '0.038355', 'train_acc': '98.715998'}]
[Epoch:0567/0600 lr:0.001417]: 100%|██| 521/521 [00:36<00:00, 14.31it/s, log={'train_loss': '0.040234', 'train_acc': '98.635997'}]
[Epoch:0568/0600 lr:0.001375]: 100%|██| 521/521 [00:36<00:00, 14.30it/s, log={'train_loss': '0.037113', 'train_acc': '98.781998'}]
[Epoch:0569/0600 lr:0.001333]: 100%|██| 521/521 [00:37<00:00, 13.81it/s, log={'train_loss': '0.039952', 'train_acc': '98.635997'}]
[Epoch:0570/0600 lr:0.001292]: 100%|██| 521/521 [00:37<00:00, 13.90it/s, log={'train_loss': '0.038527', 'train_acc': '98.699997'}]
[Val_Accuracy epoch:570] val_loss:0.510932, val_acc:88.589997
[Epoch:0571/0600 lr:0.001250]: 100%|██| 521/521 [00:38<00:00, 13.61it/s, log={'train_loss': '0.037085', 'train_acc': '98.785998'}]
[Epoch:0572/0600 lr:0.001208]: 100%|██| 521/521 [00:38<00:00, 13.61it/s, log={'train_loss': '0.037187', 'train_acc': '98.725998'}]
[Epoch:0573/0600 lr:0.001167]: 100%|██| 521/521 [00:35<00:00, 14.52it/s, log={'train_loss': '0.038730', 'train_acc': '98.697998'}]
[Epoch:0574/0600 lr:0.001125]: 100%|██| 521/521 [00:37<00:00, 13.90it/s, log={'train_loss': '0.037768', 'train_acc': '98.723997'}]
[Epoch:0575/0600 lr:0.001083]: 100%|██| 521/521 [00:36<00:00, 14.35it/s, log={'train_loss': '0.039103', 'train_acc': '98.703998'}]
[Val_Accuracy epoch:575] val_loss:0.498318, val_acc:89.079997
[Epoch:0576/0600 lr:0.001042]: 100%|██| 521/521 [00:36<00:00, 14.35it/s, log={'train_loss': '0.038639', 'train_acc': '98.697997'}]
[Epoch:0577/0600 lr:0.001000]: 100%|██| 521/521 [00:35<00:00, 14.54it/s, log={'train_loss': '0.038635', 'train_acc': '98.717998'}]
[Epoch:0578/0600 lr:0.000958]: 100%|██| 521/521 [00:36<00:00, 14.23it/s, log={'train_loss': '0.036377', 'train_acc': '98.803997'}]
[Epoch:0579/0600 lr:0.000917]: 100%|██| 521/521 [00:38<00:00, 13.71it/s, log={'train_loss': '0.037486', 'train_acc': '98.733998'}]
[Epoch:0580/0600 lr:0.000875]: 100%|██| 521/521 [00:36<00:00, 14.24it/s, log={'train_loss': '0.037114', 'train_acc': '98.751997'}]
[Val_Accuracy epoch:580] val_loss:0.545997, val_acc:88.209997
[Epoch:0581/0600 lr:0.000833]: 100%|██| 521/521 [00:38<00:00, 13.68it/s, log={'train_loss': '0.036674', 'train_acc': '98.751997'}]
[Epoch:0582/0600 lr:0.000792]: 100%|██| 521/521 [00:36<00:00, 14.34it/s, log={'train_loss': '0.036674', 'train_acc': '98.789998'}]
[Epoch:0583/0600 lr:0.000750]: 100%|██| 521/521 [00:37<00:00, 13.83it/s, log={'train_loss': '0.036661', 'train_acc': '98.775998'}]
[Epoch:0584/0600 lr:0.000708]: 100%|██| 521/521 [00:36<00:00, 14.47it/s, log={'train_loss': '0.038428', 'train_acc': '98.687998'}]
[Epoch:0585/0600 lr:0.000667]: 100%|██| 521/521 [00:37<00:00, 14.03it/s, log={'train_loss': '0.037202', 'train_acc': '98.751998'}]
[Val_Accuracy epoch:585] val_loss:0.533871, val_acc:88.339998
[Epoch:0586/0600 lr:0.000625]: 100%|██| 521/521 [00:35<00:00, 14.60it/s, log={'train_loss': '0.035556', 'train_acc': '98.811998'}]
[Epoch:0587/0600 lr:0.000583]: 100%|██| 521/521 [00:37<00:00, 14.01it/s, log={'train_loss': '0.036910', 'train_acc': '98.779998'}]
[Epoch:0588/0600 lr:0.000542]: 100%|██| 521/521 [00:36<00:00, 14.20it/s, log={'train_loss': '0.036913', 'train_acc': '98.759997'}]
[Epoch:0589/0600 lr:0.000500]: 100%|██| 521/521 [00:37<00:00, 13.79it/s, log={'train_loss': '0.036047', 'train_acc': '98.777997'}]
[Epoch:0590/0600 lr:0.000458]: 100%|██| 521/521 [00:36<00:00, 14.10it/s, log={'train_loss': '0.035996', 'train_acc': '98.817997'}]
[Val_Accuracy epoch:590] val_loss:0.556964, val_acc:88.609997
[Epoch:0591/0600 lr:0.000417]: 100%|██| 521/521 [00:36<00:00, 14.19it/s, log={'train_loss': '0.037176', 'train_acc': '98.791997'}]
[Epoch:0592/0600 lr:0.000375]: 100%|██| 521/521 [00:36<00:00, 14.27it/s, log={'train_loss': '0.034718', 'train_acc': '98.871997'}]
[Epoch:0593/0600 lr:0.000333]: 100%|██| 521/521 [00:36<00:00, 14.36it/s, log={'train_loss': '0.036348', 'train_acc': '98.771998'}]
[Epoch:0594/0600 lr:0.000292]: 100%|██| 521/521 [00:37<00:00, 13.96it/s, log={'train_loss': '0.037516', 'train_acc': '98.743998'}]
[Epoch:0595/0600 lr:0.000250]: 100%|██| 521/521 [00:36<00:00, 14.43it/s, log={'train_loss': '0.035994', 'train_acc': '98.807998'}]
[Val_Accuracy epoch:595] val_loss:0.479489, val_acc:88.929998
[Epoch:0596/0600 lr:0.000208]: 100%|██| 521/521 [00:36<00:00, 14.42it/s, log={'train_loss': '0.036818', 'train_acc': '98.773997'}]
[Epoch:0597/0600 lr:0.000167]: 100%|██| 521/521 [00:36<00:00, 14.34it/s, log={'train_loss': '0.037339', 'train_acc': '98.759997'}]
[Epoch:0598/0600 lr:0.000125]: 100%|██| 521/521 [00:35<00:00, 14.57it/s, log={'train_loss': '0.036449', 'train_acc': '98.741997'}]
[Epoch:0599/0600 lr:0.000083]: 100%|██| 521/521 [00:35<00:00, 14.69it/s, log={'train_loss': '0.035965', 'train_acc': '98.793998'}]
[Epoch:0600/0600 lr:0.000042]: 100%|██| 521/521 [00:37<00:00, 13.96it/s, log={'train_loss': '0.037097', 'train_acc': '98.767998'}]
[Val_Accuracy epoch:600] val_loss:0.534037, val_acc:88.969997
Elapsed time: hour: 6, minute: 15, second: 0.617875